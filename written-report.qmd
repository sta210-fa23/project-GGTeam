---
title: "Predicting Tik-Tok User Data Based on Video Data"
author: "GGteam: Will Chen, Katelyn Cai, Hannah Choi, Weston Slayton"
date: "12/1/23"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data
#| warning: false
#| message: false
library(dplyr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(car)
library(knitr)
library(yardstick)
library(broom)
library(recipes)
```

```{r}
tiktok <- read.csv("data/top_users_vids.csv")
```

### Introduction and data

With over 1 billion users globally, TikTok is one of the fastest growing social platforms in the world. Understanding ubiquitous algorithm, which is said to generally account for account factors (likes and comments) and video information (captions, sounds, hashtags), is critical to understanding the app's many critiques, from [declining youth mental health outcomes](https://www.amnesty.org/en/latest/news/2023/11/tiktok-risks-pushing-children-towards-harmful-content/#:~:text=TikTok%20can%20lead%20you%20to%20very%20dark%20places&text=We%20found%20that%20within%2020,to%20depression%20and%20self%20harm.&text=Take%20urgent%20measures%20now%20to,rabbit%20holes%20of%20harmful%20content.) and its [addictive nature of its explore page](https://sites.brown.edu/publichealthjournal/2021/12/13/tiktok/). To better understand TikTok's social impact, we decided to explore TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account, like average number of videos, average number of likes, and average number of comments.Â 

The dataset comes from the 'top_users_vids.csv' file of this [Github repository](https://github.com/ivantran96/TikTok_famous/tree/main), which was originally collected as part of the DataResolutions's Data Blog project exploring Tiktok's demographics and trending video analytics. The original data curators collected the data using David Teather's open-source [Unofficial Tiktok API](https://github.com/davidteather/TikTok-Api), which uses Python to scrape Tiktok data and fetch the most trending videos, specific user information, and much more. The curators expanded the list of users by collecting suggested users with the API's getSuggestedUsersbyIDCrawler method. Using byUsername, they then collected video data of the 25 most recent posts of each user. They also used the bySound method to collect videos using some of the most famous songs on TikTok.

### EDA

We begin our EDA process by first examining the dataset.

```{r}
#| include: false
names(tiktok)
```

Currently, our dataset tiktok has 13 columns and 12,559 observations. Each row is a video. The columns cover attributes of each video such as video length, hashtags used, songs/sounds used, and statistics (number of likes, shares, comments, plays, followers, and total number of likes and videos across the account). Variables id, create_time, video_length, n_likes, n_shares, n_comments, n_plays, n_followers, n_total_likes, and n_total_vids are numerical while the others are categorical.

It is apparent that we must address the potential issue user_name might have with the other columns. If we simply drop it, we could face multicollinearity, where the number of plays or likes on a video is closely tied to the user who posted it. So, excluding information about the user could lead to incomplete analysis, as the user-related factors become confounding variables.

The less relevant variables are create time and video ID. In addition, hashtags and songs might not be useful. Most videos don't include a hashtag and there are too many unique instances of them for it to be valuable in our analysis. We could consider binning hashtag into none and at least 1 hashtag(s), however that wouldn't be useful for our analysis since its rare for tiktok followers to mind the number of hashtags. The same is true for songs; it wouldn't be useful to categorize all original songs as similar since most of them could just be user-edited snippets of actual songs.

To address the issues mentioned above, we grouped the data by users and summarized relevant predictor variables by taking their mean. Our modified dataset has 8 columns and 254 observations, with each row being a user. No data leakage is introduced in this process since we are just summarizing by the means of the predictor variable per user. When this completed, we split our dataset into training and test sets:

```{r}
#| include: false
tiktok_users <- tiktok |>
  dplyr::group_by(user_name)|>
  dplyr::summarize(likes = mean(n_likes),
            shares = mean(n_shares),
            comments = mean(n_comments),
            plays = mean(n_plays),
            followers = mean(n_followers),
            video_length = mean(video_length),
            total_videos = mean(n_total_vids))

head(tiktok_users, 5)
```

```{r}
set.seed(29)

tiktok_split <- initial_split(tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

Here's a distribution of our response variable, user followers, from our training set.

```{r}
#| out.width="80%", out.height="80%"
tiktok_train |>
  ggplot(aes(x = followers/1000000)) + 
  geom_histogram() + 
  labs(x = "Followers (in Millions)", y = "Count", title = "Distribution of Number of Followers", bins=100) +
  scale_x_continuous(labels = label_number()) + 
  theme(aspect.ratio = .5, 
        plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))
```

```{r}
#| echo: FALSE
#| eval: false
min_followers <- min(tiktok_train$followers, na.rm = TRUE)
max_followers <- max(tiktok_train$followers, na.rm = TRUE)
mean_followers <- mean(tiktok_train$followers, na.rm = TRUE)
sd_followers <- sd(tiktok_train$followers, na.rm = TRUE)
print(paste("Mean of followers:", mean_followers))
print(paste("Standard deviation of followers:", sd_followers))
print(paste("Minimum number of followers:", min_followers))
print(paste("Maximum number of followers:", max_followers))
```

The distribution of our response variable is unimodal and heavily right skewed. The mean and standard deviation are 16,220,526 and 7,710,869, respectively. And the min and max are 8,900,000 and 52,300,000, respectively. This means our dataset contains the upper range of users in terms of followers.

Here are the distributions for the predictor variables we are interested in:

```{r}
#| out.width="60%", out.height="60%"

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos", "video_length")

long_train <- tiktok_train |>
  select(all_of(predictor_vars)) |>
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(long_train, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(x = "Value", y = "Count", title = "Distribution of Predictor Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_continuous(labels = scales::comma)+
  theme(plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))

```

```{r}
transformed_tiktok_users <- tiktok |>
  dplyr::group_by(user_name) |>
  dplyr::summarize(
    likes = mean(n_likes),
    shares = mean(n_shares),
    comments = mean(n_comments),
    plays = mean(n_plays),
    followers = mean(n_followers),
    video_length = mean(video_length),
    total_videos = mean(n_total_vids)
  ) |>
  mutate(
#    followers = log(followers),
    comments = comments / 1000,         # Scale comments to hundreds
    likes = likes / 1000000,            # Scale likes to millions
    plays = plays / 1000000,            # Scale plays to tens of millions
    shares = shares / 1000,             # Scale shares to hundreds
    total_videos = total_videos / 1000  # Scale total_videos to units
  ) |>
  select(-user_name)

set.seed(29)

tiktok_split <- initial_split(transformed_tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

```{r}
tiktok_recipe <- recipe(followers ~ ., data = tiktok_train) |>
  step_center(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_discretize(video_length, options = list(cuts = 3)) 
  
prepped_tiktok_recipe <- prep(tiktok_recipe, training = tiktok_train)
train_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_train)
test_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_test)
```

### **Methodology**

We are using multiple linear regression to predict the number of followers a user has, over logistic regression because followers is a quantiative response variable. We start off with an initial model containing the predictor's likes, shares, comments, plays, video_length (factor with 3 levels), total_videos, and followers. Because Tiktok videos are commonly divided into 15-second, 1 minute, or 3 minute videos, we bin average video length into 3 levels, corresponding to "short", "medium" and "long." We also mean-center all our numerical variables to make our intercept meaningful, and we scale comments to hundreds, likes to millions, plays to tens of millions, shares to hundreds, and total_videos to units for increased interpretability. Here is a tidy table of our initial model:

```{r initial-model}

initial_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ ., data=train_transformed)

initial_fit |>
  tidy() |>
  kable(digits = 4)
```

#### Conditions for Inference

In assessing linearity and constant variance, it is important to look at Residual vs. Fitted plots for quantitative predictor variables (likes, shares, comments, plays, and total vidoes) and look for patterns and fanning:

```{r}
#| out.width="80%", out.height="80%"

create_individual_residual_plot <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("avg", predictor, "per user")) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 5),
      axis.text.y = element_text(size = 5),
      plot.title = element_text(size = 8),
      axis.title = element_blank(), 
      axis.ticks = element_blank(), 
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")
individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, train_transformed))

combined_plots <- reduce(individual_residual_plots, `+`)

final_plot <- combined_plots + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

We can see from the residual plots that there doesn't appear to be any non-random patterns that violate linearity. Therefore, we can conclude that the linearity condition is satisfied. However, there does appear to be a clear outward fanning spread for each each predictor, meaning that constant variance is not satisfied. To resolve this, we log-transform our response variable (followers) and see if the fanning is minimized:

```{r}
#| out.width="80%", out.height="80%"
train_transformed <- train_transformed |>
  mutate(followers = log(followers))

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, train_transformed))

combined_residual_plot <- wrap_plots(individual_residual_plots, ncol = 2)

final_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

We can see that after log-transforming followers, the scale of our y axis decreases significantly. There is no clear outward fanning, but rather a lower density of points as you move to higher values on the x-axis. As such, we conclude the predictors satisfy constant variance. When assessing independence, we know that each of the videos are by individual creators, therefore it is likely the videos were produced independently of each other. There is no reason to believe that one TikTok user's video performance would directly affect another's.

Finally, we assess normality by looking at the residual histograms for each predictor:

```{r}
#| out.width="60%", out.height="60%"

create_individual_residual_histogram <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .resid)) +
    geom_histogram(bins = 40, fill = "blue", color = "black", alpha = 0.5) +
    labs(title = paste(predictor)) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 5),
      axis.text.y = element_text(size = 5),
      plot.title = element_text(size = 12, face = "bold"),
      axis.title.x = element_blank(), 
      axis.title.y = element_blank(), 
      axis.ticks = element_blank(),
    )
}

predictor_vars <- c("likes", "shares", "total_videos", "video_length", "comments", "plays")

individual_residual_histograms <- map(predictor_vars, ~create_individual_residual_histogram(.x, tiktok_users))

combined_residual_plot <- wrap_plots(individual_residual_histograms, ncol = 2)

final_hist_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residual Distribution for Each Predictor",
    caption = "Residual",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_hist_plot <- final_hist_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_hist_plot) +
  labs(tag = "count") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

Normality doesn't seem to be satisfied for all of the predictors. However, because we have more than 30 observations in the dataset, we can conclude that normality is satisfied regardless of the distribution.

#### Detecting Multicollinearity & Model Comparison

Upon conducting a VIF test, we found that likes and plays had the highest vif values (11.614 and 9.82 respectively):

```{r}
tiktok_users_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ ., data=train_transformed)

vif(tiktok_users_fit$fit) |>
  kable(digits = 3)
```

Therefore, we wanted to assess which model would perform better: a model without likes or a model without plays. To do this, we performed 5-fold cross validation and extracted the resulting AIC, BIC, adj.r-squared, and RMSE values for the two models:

```{r}
set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)

calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}

tiktok_recipe1 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(likes)

tiktok_recipe2 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(plays)

tiktok_wflow1 <- workflow() |>
  add_recipe(tiktok_recipe1) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_wflow2 <- workflow() |>
  add_recipe(tiktok_recipe2) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs1 <- tiktok_wflow1 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

tiktok_fit_rs2 <- tiktok_wflow2 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

Model 1: (without likes):

```{r}
# extract RMSE
rmse <- collect_metrics(tiktok_fit_rs1, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

# extract adjusted R-squared, AIC, and BIC
other_metrics <- map_df(tiktok_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics <- bind_cols(rmse, other_metrics) 
combined_metrics
```

Model 2 (without plays):

```{r}
# do the same for model 2
rmse2 <- collect_metrics(tiktok_fit_rs2, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics2 <- map_df(tiktok_fit_rs2$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics2 <- bind_cols(rmse2, other_metrics2)
combined_metrics2
```

The difference between the model's evaluations aren't large. Model 1 has a higher RMSE, while it has a lower AIC and BIC, and a higher adjusted r-squared. In this case, we would consider model 2 (the model without plays) to be a better model, because it has a lower RMSE, which is gathered from the assessment set and is used to assess prediction. The goal of our model is to predict followers, so we want to choose the model with better predictive power (Model 2). Therefore, we remove plays from our model.

#### Determining whether video_length_bin are necessary

We saw from our initial tidy table that the p-values associated with video length bins are high, indicating that the variables may not be significant. Because of this, we can once again perform cross validation to test how a model without video_length compares to our current model (Model 1):

Model 3 (without plays and video length):

```{r}
set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)
# cv against previously chosen model and model without video length
tiktok_recipe3 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(plays) |>
  step_rm(video_length)

tiktok_wflow3 <- workflow() |>
  add_recipe(tiktok_recipe3) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs3 <- tiktok_wflow3 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

```

```{r}
rmse3 <- collect_metrics(tiktok_fit_rs3, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics3 <- map_df(tiktok_fit_rs3$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics3 <- bind_cols(rmse3, other_metrics3)
combined_metrics3
```

We can see that when we remove video_length, RMSE is slightly higher than it was for Model 2, while AIC remains about the same and BIC slightly decreases. We also see that adjusted r-squared remains about the same. Therefore, despite BIC slightly decreasing, we prefer the model with a lower RMSE (better prediction), so we don't want to remove video_length from our model.

#### Determining whether interaction terms are needed

Our only categorical variable in our model is video_length. Therefore, we chose to assess all possible interaction terms with video_length for statistical significance:

```{r}
model_with_interaction <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~  likes + shares + comments + total_videos + video_length + video_length*(likes + shares + comments + total_videos), data = train_transformed)

model_with_interaction |>
  tidy() |>
  kable(digits = 3)
```

We can see from the table that all variables are significant when interacting with video_lengthbin3 (p-value is less than significance level of 0.05) except for total_videos. Because of this, we know that we won't need to include the interaction term between total_videos and video_length. Additionally, given that comments has a high p-value in this new model, we can try removing comments from our model as well. We can use cross validation to test how a model without comments, and with video_length interacting with shares and likes performs compared to our current model (Model 2):

```{r}
#| message: false
tiktok_recipe4 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(plays) |>
  step_interact(~ likes:video_length) |>
  step_interact(~ shares:video_length) |>
  step_rm(comments)
  

tiktok_wflow4 <- workflow() |>
  add_recipe(tiktok_recipe4) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs4 <- tiktok_wflow4 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

```{r}

rmse4 <- collect_metrics(tiktok_fit_rs4, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics4 <- map_df(tiktok_fit_rs4$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics4 <- bind_cols(rmse4, other_metrics4)
combined_metrics4
```

We can see that RMSE significantly decreased from about 6.6 million in Model 2 to 6.4 million in Model 4. We also see that adjusted r-squared increased, AIC decreased, and BIC decreased. All of these signs point to Model 4 being a better model in both fit and prediction. Therefore, we will remove comments, and add interactions between video_length and both shares and likes.

### Results

After removing plays and comments and adding interaction terms between video_length and both likes and shares, we arrive at our final model:

```{r}
final_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ shares + likes + total_videos + likes*video_length + shares*video_length, data=train_transformed)

final_fit |>
  tidy() |>
  kable(digits = 4)
```

We now check our final model conditions:

```{r}
tiktok_fit4 <- tiktok_wflow4 |>
  fit(data = train_transformed)

tiktok_test_pred4 <- predict(tiktok_fit4, train_transformed) |>
  bind_cols(train_transformed) |>
  mutate(residuals = followers - .pred)

residuals_vs_fitted_plot <- ggplot(tiktok_test_pred4, aes(x = .pred, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dotted") +
  labs(x = "Fitted Values", y = "Residuals", title = "Residuals vs Fitted Values") +
  theme_minimal()

residuals_distribution_plot <- ggplot(tiktok_test_pred4, aes(x = residuals)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  geom_vline(aes(xintercept = mean(residuals)), color = "red", linetype = "dotted") +
  labs(x = "Residuals", y = "Count", title = "Distribution of Residuals") +
  theme_minimal()

residuals_vs_fitted_plot <- residuals_vs_fitted_plot +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))

residuals_distribution_plot <- residuals_distribution_plot +
  theme(plot.title = element_text(size = 10),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8))
```

```{r}
#| out.width="70%", out.height="40%"
residuals_vs_fitted_plot + residuals_distribution_plot
```

Linearity is met because there's a random scatter across the horizontal axis in the residual plot. Normality is satisfied because we have more than 30 observations in the dataset. Independence is satisfied, as we still have no reason to believe the performance of one user's videos would impact another. Constant variance is satisfied because there is an even distribution of datapoints across residual model's y-axis.

Final Model performance on testing set:

```{r}
# predict on test
test_transformed <- test_transformed |>
  mutate(followers = log(followers))
  
tiktok_test_pred4 <- predict(final_fit, test_transformed) |>
  bind_cols(test_transformed)
  
rmse_result4 <- rmse(tiktok_test_pred4, truth = followers, estimate = .pred)
rsq_result4 <- rsq(tiktok_test_pred4, truth = followers, estimate = .pred)

combined_metrics4 <- bind_rows(rmse_result4, rsq_result4)
combined_metrics4 |>
  kable(digits = 3)
```

Note that we log transformed our response variable. In order to evaluate the meaning of our RMSE of 0.510, we take exp(0.510) \~ 1.665. This value is the multiplicative square difference. For example, if have log followers of 16.04552, our model will be more or less off by $16.04552 \pm 0.510^2$ $\implies \exp(16.04552 \pm 0.2601) \implies 7170028 < 9,299,954 < 12,062,597$ (this is evaluated using average sum of square difference, so the range would be larger). This means our model does a fairly poor at predicting a tiktok user's followers. We also have an RSQ of 0.177, indicating only 17.7% of the variability in followers can be explained by our predictor variables.

Our interpretation for the intercept is as follows: We expect the tiktok user with the mean number of likes, shares, total_videos, and with an average video length belonging to bin_length1 to have around 14,590,774 followers (exp\[16.4959\]), on average.

We then examined likes, which is the predictor with the largest effect on user followers: for every additional million average likes of a user, we expect the expected to multiply by a factor of about 1.481(exp\[0.3925\]), compared to the baseline with mean number of shares, total videos, and with an average video length belonging to the bottom third of video lengths, holding all else constant.

There are several terms that are significant when determining the number of followers a tik_tok user has. The number of total videos, comments, and plays seems to have a clear positive relationship with follower count. This also would align with our expectations, as the more videos you make, the more engagement your profile is likely to have and more followers you may gain. However, shares have a negative relationship with follower count, which initially seemed counter-intuitive. While it is impossible for the model to determine causality or explain why exactly a relationship exists, we hypothesize that users may share a video because they dislike it, resulting in them not following the user.

When observing the video length bin variable, the longest video length bin (3) has statistically significant difference from the other two video length bins and a statistically significant interaction term with likes and shares. This suggests that a higher average video length of a user has may effect the user's followers count more significantly.

### Discussion + Conclusion

We originally decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account. We learned that it is extremely difficult to correctly predict follower count, given our model only captures 17.7% of the variability in the dataset. Even still, we found it interesting that users with longer average videos lengths has a negative effect on the average likes a user could have. For example, in our final model, the coefficient for likes:video_bins3 is -0.344, which is a significant decrease from the baseline with likes:video_bins1 at 0.3925.

Our dataset was extremely difficult to work with, given that it did not meet the conditions for linear regression (linearity and constant variance), and contained multicollinearity. The variables were also extremely large, and needed to be scaled down to have meaningful coefficients - which made interpretations significantly more difficult. A more complex model is likely needed, given how poorly our model performed at the end. There also may be underlying relationships between follower count, and other portions of the TikTok algorithm that are not contained in the dataset, which our model might have also failed to capture; in the real world, users have reported that TikTok enforces policies differently from user-to-user, and uses different algorithms from region to region.

In order to improve our analysis, it would be helpful to comb TikTok for a dataset that potentially contains more variables. Three potential options we considered included: finding a meaningful way to capture hashtags (which may require manually looking at TikTok videos), finding a meaningful way to capture whether a user typically utilizes trending music, and using the demographic statistics for users (to account for bias in human decision making).

### Appendix

First 5 data points before transformation

```{r}
head(tiktok)
```

After transformation

```{r}
head(tiktok_users, 5)
```

Split results:

```{r}
tiktok_split
```
