---
title: "Predicting Tik-Tok User Data Based on Video Data"
author: "GGteam: Will Chen, Katelyn Cai, Hannah Choi, Weston Slayton"
date: "12/1/23"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data
#| warning: false
#| message: false
library(dplyr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(car)
library(knitr)
library(yardstick)
library(broom)
library(recipes)
```

```{r}
tiktok <- read.csv("data/top_users_vids.csv")
```

#### Introduction and data

TikTok now has over 1 billion users globally, making it one of the fastest growing social platforms in the world. As it has risen to prominence, so has its ubiquitous algorithm, which is said to generally account for account factors (likes and comments) and video information (captions, sounds, hashtags). Given, that TikTok has been heavily criticized alongside other platforms for declining youth mental health outcomes and rising hate due to the addictive nature of its explore page, we decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account, like average number of videos, average number of likes, and average number of comments. 

The dataset comes from the 'top_users_vids.csv' file (under folder 'Trending Videos Data Collection') of the Github repository found at: https://github.com/ivantran96/TikTok_famous/tree/main. The data was originally collected as part of the DataResolutions's Data Blog project exploring Tiktok's demographics and trending video analytics. 

The original data curators collected the data using David Teather's open-source Unofficial Tiktok API (found at https://github.com/davidteather/TikTok-Api), which uses Python to scrape Tiktok data and fetch the most trending videos, specific user information, and much more. Using the list of top Tiktokers, the curators compiled a list of users with the getSuggestedUsersbyIDCrawler api method, which used the top TikTokers and collected the suggested users. Using the byUsername method, they collected video data of the 25 most recent posts of each user from the top TikTokers and the suggested list. The curators also used the API's bySound method to collect videos using some of the most famous songs on TikTok to get an idea of how the choice of music can impact the potential of a video to become a trending video.

#### **EDA**

We begin our EDA process by first examining the dataset.

```{r}
#| include: false
names(tiktok)
```

Currently, our dataset tiktok has 13 columns and 12,559 observations. The columns cover attributes of a tiktok video such as video length, hashtags used, songs/sounds used, and number of likes, shares, comments, plays, and followers (and their total number of likes and videos). Variables id, create_time, video_length, n_likes, n_shares, n_comments, n_plays, n_followers, n_total_likes, and n_total_vids are numerical while the others are categorical.

However, from just our initial exploration, it's clear that some of the columns won't be useful for our analysis. It is also apparent that we should find a way to address the potential issue user_name might have with the other columns. There's a potential for severe multicollinearity if we choose to just drop user_name, since the number of plays or likes a video would have a strong relationship with the user that post it. Therefore, any analysis without user and its related features would have to consider the user's account as confounding variables. In addition, we'll be forced to drop valuable features directly related to a user such as user followers, user total likes and user total videos (n_followers, n_total_likes, n_total_vids).

We see that the less relevant variables are create time and video ID. In addition, from looking at our data, hash tags and songs might not be useful. Most videos don't include a hashtag and there are too many unique instances of them for it to be valuable in our analysis. We could consider binning hashtag into none and at least 1 hashtag(s), however that wouldn't be useful for our analysis since its rare for tiktok followers to actually look at the hashtags. The same is true for songs; one could consider grouping original songs into one bin and the rest into others. However, from our domain knowledge, its wouldn't be useful to categorize all original songs as similar since most of them could just be user-edited snippets of actual songs.

To address the issues mentioned above, we grouped the data by users and summarized all relevant predictor variables by taking their mean. Our modified dataset now has 8 columns and 254 observations.

```{r}
#| include: false
tiktok_users <- tiktok |>
  dplyr::group_by(user_name)|>
  dplyr::summarize(likes = mean(n_likes),
            shares = mean(n_shares),
            comments = mean(n_comments),
            plays = mean(n_plays),
            followers = mean(n_followers),
            video_length = mean(video_length),
            total_videos = mean(n_total_vids))

head(tiktok_users, 5)
```

Note that no data leakage is introduced in this process since we are just summarizing by the means of the predictor variable per user. When we split, it'll split based on observations, users. We'll now split our data into testing and training.

```{r}
set.seed(29)

tiktok_split <- initial_split(tiktok_users, prop = 0.75)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
tiktok_split
```

Here's a distribution of our response variable, user followers, from our training set.

```{r}
tiktok_train |>
  ggplot(aes(x = followers)) + 
  geom_histogram() + 
  labs(x = "Followers", y = "Count", title = "Distribution of Number of Followers", bins=100) + 
  scale_x_continuous(labels = label_number()) + 
  theme(aspect.ratio = .5, 
        plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))
```

```{r}
#| echo: FALSE
#| eval: false
min_followers <- min(tiktok_train$followers, na.rm = TRUE)
max_followers <- max(tiktok_train$followers, na.rm = TRUE)
mean_followers <- mean(tiktok_train$followers, na.rm = TRUE)
sd_followers <- sd(tiktok_train$followers, na.rm = TRUE)
print(paste("Mean of followers:", mean_followers))
print(paste("Standard deviation of followers:", sd_followers))
print(paste("Minimum number of followers:", min_followers))
print(paste("Maximum number of followers:", max_followers))
```

The distribution of our response variable follows, is unimodal and heavily right skewed. The mean is 16,220,526.3 and the standard deviation is 7,710,869.8. The minimum is 8,900,000 and the maximum is 52,300,000. Based on our standard deviation, there seems to be a lot of variation in our dataset; and from our plot, there are major outliers.

Here are the distributions for the predictor variables we are interested in:

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos", "video_length")

long_train <- tiktok_train |>
  select(all_of(predictor_vars)) |>
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(long_train, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(x = "Value", y = "Count", title = "Distribution of Predictor Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_continuous(labels = scales::comma)+
  theme(plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))

```

juging by the number of outliers in our dataset, we are interesting in knowing how this might influence our model conditions. Hence, we have the following residual plots for each of them.

```{r}
create_individual_residual_plot <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("avg", predictor, "per user")) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 5),
      axis.text.y = element_text(size = 5),
      plot.title = element_text(size = 8),
      axis.title = element_blank(), 
      axis.ticks = element_blank(), 
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")
individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, tiktok_users))

combined_plots <- reduce(individual_residual_plots, `+`)

final_plot <- combined_plots + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )

```

It's clear that all our predictors variables violates constant variance. There's a clear outward spread for likes, shares, comments, plays and video_length, while an inward spread for total_videos.

For interpretability, it makes sense to process average bin video length into levels, corresponding to "short", "medium" and "long." This also allows us to search for interesting interactions effects video_length might have with other predictors such as likes. Therefore, we'll add step_discretize() into the recipe for video_length.

In order to deal with constant variance for the other terms, we log transformed the rest of the predictor variables.

```{r}
transformed_tiktok_users <- tiktok |>
  dplyr::group_by(user_name) |>
  dplyr::summarize(
    likes = mean(n_likes),
    shares = mean(n_shares),
    comments = mean(n_comments),
    plays = mean(n_plays),
    followers = mean(n_followers),
    video_length = mean(video_length),
    total_videos = mean(n_total_vids)
  ) |>
  mutate(
    followers = log(followers),
  ) |>
  select(-user_name)

set.seed(29)

tiktok_split <- initial_split(transformed_tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

```{r}
tiktok_recipe <- recipe(followers ~ ., data = tiktok_train) |>
  step_center(all_numeric_predictors()) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors())

prepped_tiktok_recipe <- prep(tiktok_recipe, training = tiktok_train)
train_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_train)
test_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_test)
```

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")  

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, train_transformed))

combined_residual_plot <- wrap_plots(individual_residual_plots, ncol = 2)

final_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

Fanning is a lot less noticeable now. The residual plots don't seem to suggest any underlying patterns. As such, we conclude the predictors satisfy linearity and constant variance.

We can also assume independence is met. Each of the videos are by individual creators, therefore the videos are independent of each other.

We continue with normality.

```{r}
create_individual_residual_histogram <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .resid)) +
    geom_histogram(bins = 40, fill = "blue", color = "black", alpha = 0.5) +
    labs(title = paste(predictor)) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 5),
      axis.text.y = element_text(size = 5),
      plot.title = element_text(size = 12, face = "bold"),
      axis.title.x = element_blank(), 
      axis.title.y = element_blank(), 
      axis.ticks = element_blank(),
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")

individual_residual_histograms <- map(predictor_vars, ~create_individual_residual_histogram(.x, tiktok_users))

combined_residual_plot <- wrap_plots(individual_residual_histograms, ncol = 2)

final_hist_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residual Distribution for Each Predictor",
    caption = "Residual",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_hist_plot <- final_hist_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_hist_plot) +
  labs(tag = "count") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

Normality seems to be satisfied for each of the predictors except total_videos and possibly shares. However, even though total_videos and shares do not have completely normal distributions, because we have more than 30 observations in the dataset, we can conclude that normality is satisfied regardless of the distribution.

#### **Methodology**

We chose the model without plays, m2, because it had a lower AIC and BIC value, indicating that it was a better fit. Therefore, we choose to remove plays from the model and leave likes in the model to deal with the multicollinearity. 

In our recipe, we fit the dataset 'tiktok_user,' after which we added steps omitting all NA values from our predictors and log-transforming all our predictor variables and followers. We then conducted a cross-validation test on tiktok_train.

#### Detecting Multicollinearity & Model Comparison

In our model selection process, we are interested in detecting any multicollinearity. We found that likes and plays had the highest vif values in our VIF test (11.614 and 9.82 respectively).

```{r}
tiktok_users_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ likes + shares + comments + plays + video_length_bin2 + video_length_bin3 + total_videos, data=train_transformed)

vif(tiktok_users_fit$fit)
```

Therefore, we constructed two linear regression model fitting variable 'followers' with predictor variables 'likes', 'shares', 'comments', 'plays', 'video_length_bin', and 'total_videos.' Our first model m1 had all the previously indicated predictor variables excluding variable 'likes' while our second model m2 had those predictor variables excluding variable 'plays.' We compared these two models because meaning they have the highest likelihood for multicollinearity.

```{r}
m1 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - likes, data = train_transformed)
  
m1 |>
  tidy() |>
  kable(digits = 3)
```

```{r}
m2 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - plays, data = train_transformed)
  
m2 |>
  tidy() |>
  kable(digits = 3)
```

**Model Comparison with 5-fold CV**

To determine which between m1 and m2, we preform 5-fold cross validation and extract the resulting BIC and AIC scores along with additional evaluations.

```{r}

set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)

calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}

tiktok_recipe1 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(likes) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes()) |>
  step_normalize(all_numeric_predictors())

tiktok_recipe2 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(plays) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes()) |>
  step_normalize(all_numeric_predictors())

tiktok_wflow1 <- workflow() |>
  add_recipe(tiktok_recipe1) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_wflow2 <- workflow() |>
  add_recipe(tiktok_recipe2) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs1 <- tiktok_wflow1 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

tiktok_fit_rs2 <- tiktok_wflow2 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

Model 1: (without likes):

RMSE:

```{r}
# extract RMSE
rmse <- collect_metrics(tiktok_fit_rs1, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

# extract adjusted R-squared, AIC, and BIC
other_metrics <- map_df(tiktok_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics <- bind_cols(rmse, other_metrics) 
combined_metrics
```

Model 2 (without plays):

RMSE:

```{r}
# do the same for model 2
rmse2 <- collect_metrics(tiktok_fit_rs2, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics2 <- map_df(tiktok_fit_rs2$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics2 <- bind_cols(rmse2, other_metrics2)
combined_metrics2
```

The difference between the model's evaluations aren't large. Model 1 has a slightly higher RMSE, but it has a lower AIC and BIC, and a higher adjusted r-squared. In thise case, we would consider model 1 (the model without likes) to be a better model since it's able to explain more of the variance while also maintaining lower AIC and BIC scores. Therefore, we choose to remove likes from the model and leave plays in the model to deal with the multicollinearity.

### Determining whether video_length_bin are necessary

We can see from our previous tidy table that the p-values associated with the video length bins are high, indicating that the variables aren't significant. Because of this, we can do once again do cross validation to test to see how a model without video_length and likes performs against m1 (our chosen model without likes but contains video length).

Model 3 (without video length and likes):

```{r}
set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)
# cv against previously chosen model and model without video length
tiktok_recipe3 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(likes) |>
  step_rm(video_length) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes()) |>
  step_normalize(all_numeric_predictors())

tiktok_wflow3 <- workflow() |>
  add_recipe(tiktok_recipe3) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs3 <- tiktok_wflow3 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

```

```{r}
rmse3 <- collect_metrics(tiktok_fit_rs3, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics3 <- map_df(tiktok_fit_rs3$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics3 <- bind_cols(rmse3, other_metrics3)
combined_metrics3
```

We can see that when we remove video_length, it makes sense that AIC and BIC both decrease. However, we also see that adjusted r-squared only slightly increased. We wish to further evaluation video_length and potentially tease out some of its importance for our model. Therefore, we explore its role as an interaction term.

### Determining whether interaction terms are needed

To do this, we can start by adding all of the interaction terms associated with video_length bins and observing our tidy table for significant coefficients. We will be incorporating both likes and plays into this model for a fuller evaluation of video_length as an interaction term. The end goal is to determine whether the parsimonious model of not including likes and video_length bin (model 3) is a better predictor than a model could potentially fully incorporate video_length into inself.

This is a table with all of our interaction terms:

```{r}
model_with_interaction <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ likes + plays + shares + comments + total_videos + video_length_bin2 + video_length_bin3 + video_length_bin2*(likes + shares + total_videos) + video_length_bin3*(likes + shares + total_videos), data = train_transformed)

model_with_interaction |>
  tidy(conf.int = TRUE) |>
  kable(digits = 3)
```

We can see from the p-values in the table that likes:video_length_bin3 and shares:video_length_bin3 seem to be the only statistically significant interaction terms. Therefore, those terms should certainly be in our model. All three of the interaction terms associated with video_length_bin2 have extremely high p-values and low coefficients, meaning that video_length_bin2 is insignificant. The p-value for total_videos:video_length_bin3 is not less than our significance level of 0.05, but it is still low. We can preform CV on a model with likes:video_length_bin and shares:video_length_bin.

Model 4 (with plays:video_length and likes:video_length interaction terms):

```{r}
tiktok_recipe4 <- recipe(followers ~ ., data = tiktok_train) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_interact(~ plays:video_length) |>
  step_interact(~ likes:video_length) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes()) |>
  step_normalize(all_numeric_predictors())

tiktok_wflow4 <- workflow() |>
  add_recipe(tiktok_recipe4) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs4 <- tiktok_wflow4 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

rmse4 <- collect_metrics(tiktok_fit_rs4, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics4 <- map_df(tiktok_fit_rs4$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics4 <- bind_cols(rmse4, other_metrics4)
combined_metrics4
```

The mean RMSE (0.3378707) is barely smaller than model3, one without likes and video_length_bin (0.3389576). However given we are have included more terms, our adj r-square shows a slight increase (0.2839 vs. 0.26883, the latter is the parsimonious model), and BIC shows a noticeable decrease (130.3332 vs 105.9608). There's also a difference in AIC (91.944 vs 88.243). Becuase of the noticeable decrease in AIC and BIC going from model 4 to model 3 and because our the increase in our adj r-square isn't significant enough to consider, we choose the more parsimonious model, model 3.

#### Results

```{r}
#| include: false
tiktok_fit4 <- tiktok_wflow4 |>
  fit(data = tiktok_train)
# predict on test
tiktok_test_pred4 <- predict(tiktok_fit4, tiktok_test) |>
  bind_cols(tiktok_test)
  
rmse_result4 <- rmse(tiktok_test_pred4, truth = followers, estimate = .pred)
rsq_result4 <- rsq(tiktok_test_pred4, truth = followers, estimate = .pred)

combined_metrics4 <- bind_rows(rmse_result4, rsq_result4)
combined_metrics4
```

Model 3 performance on test:

```{r}
tiktok_fit3 <- tiktok_wflow3 |>
  fit(data = tiktok_train)

tiktok_test_pred3 <- predict(tiktok_fit3, tiktok_test) |>
  bind_cols(tiktok_test)
  
rmse_result3 <- rmse(tiktok_test_pred3, truth = followers, estimate = .pred)
rsq_result3 <- rsq(tiktok_test_pred3, truth = followers, estimate = .pred)

combined_metrics3 <- bind_rows(rmse_result3, rsq_result3)
combined_metrics3
```

Note that we log transformed our response variable. In order to evaluate the meaning of our RMSE of 0.4067, we take exp(0.4067) \~ 1.502. This value is the multiplicative square difference. For example, if have log followers of 16.04552, our model will be more or less off by $16.04552 \pm .4067^2 \implies \exp(16.04552 \pm 0.1654) \implies 7882219 < 9,299,954 < 10,972,689$. This means our model does a fairly poor at predicting a tiktok user's followers. We also have an RSQ of 0.3615, indicating only 36.2% of the variability in followers can be explained by our predictor variables.

Interpreting useful coefficients:

-   we were confused about how to write a meaning interpretation out since the scale of our data is so large. We could use "for every additional likes", however a video could have millions of likes.

Interpreting intercept (since we mean centered all predictors):

#### Conclusion

We originally decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account. We learned that it is extremely difficult to correctly predict follower count, given our model only captures 36.2% of the variability in the dataset. More complex models seemed to only worsen performance, and we chose to prioritize parsimony for this reason - however, even the simple models did not predict well.

Our dataset was extremely difficult to work with, given that it did not meet the conditions for linear regression (linearity and constant variance), and contained multicollinearity. The variables were also extremely large, and needed to be scaled down to have meaningful coefficients - which made late interpretation significantly more difficult. A more complex model was likely needed, that was beyond the scope of our knowledge, given how poorly our model performed at the end. There also may be underlying relationships between follower count, and other portions of the TikTok algorithm that are not contained in the dataset, which our model might have also failed to capture. In order to improve our analysis, it would be helpful to comb TikTok for a dataset that potentially contains more variables.

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::
