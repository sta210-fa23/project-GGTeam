---
title: "Predicting Tik-Tok User Data Based on Video Data"
author: "GGteam: Will Chen, Katelyn Cai, Hannah Choi, Weston Slayton"
date: "Nov 9"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data
#| warning: false
#| message: false
library(dplyr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(car)
library(knitr)
library(yardstick)
library(broom)
library(recipes)
```

```{r}
tiktok <- read.csv("data/top_users_vids.csv")
```

#### Introduction and data

TikTok now has over 1 billion users globally, making it one of the fastest growing social platforms in the world. As it has risen to prominence, so has its ubiquitous algorithm, which is said to generally account for account factors (likes and comments) and video information (captions, sounds, hashtags). Given, that TikTok has been heavily criticized alongside other platforms for declining youth mental health outcomes and rising hate due to the addictive nature of its explore page, we decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account, like average number of videos, average number of likes, and average number of comments. 

The dataset comes from the 'top_users_vids.csv' file (under folder 'Trending Videos Data Collection') of the Github repository found at: https://github.com/ivantran96/TikTok_famous/tree/main. The data was originally collected as part of the DataResolutions's Data Blog project exploring Tiktok's demographics and trending video analytics. 

The original data curators collected the data using David Teather's open-source Unofficial Tiktok API (found at https://github.com/davidteather/TikTok-Api), which uses Python to scrape Tiktok data and fetch the most trending videos, specific user information, and much more. Using the list of top Tiktokers, the curators compiled a list of users with the getSuggestedUsersbyIDCrawler api method, which used the top TikTokers and collected the suggested users. Using the byUsername method, they collected video data of the 25 most recent posts of each user from the top TikTokers and the suggested list. The curators also used the API's bySound method to collect videos using some of the most famous songs on TikTok to get an idea of how the choice of music can impact the potential of a video to become a trending video.

#### **EDA**

We begin our EDA process by first examining the dataset. We have the following columns:

```{r}
names(tiktok)
```

Currently, our dataset tiktok has 13 columns and 12,559 observations. The columns cover attributes of a tiktok video such as video length, hashtags used, songs/sounds used, and number of likes, shares, comments, plays, and followers (and their total number of likes and videos). Variables id, create_time, video_length, n_likes, n_shares, n_comments, n_plays, n_followers, n_total_likes, and n_total_vids are numerical while the others are categorical.

However, from just our initial exploration, it's clear that some of the columns won't be useful for our analysis. It is also apparent that we should find a way to address the potential issue user_name might have with the other columns. There's a potential for severe multicollinearity if we choose to just drop user_name, since the number of plays or likes a video would have a strong relationship with the user that post it. Therefore, any analysis without user and its related features would have to consider the user's account as confounding variables. In addition, we'll be forced to drop valuable features directly related to a user such as user followers, user total likes and user total videos (n_followers, n_total_likes, n_total_vids).

We see that the less relevant variables are create time and video ID. In addition, from looking at our data, hash tags and songs might not be useful. Most videos don't include a hashtag and there are too many unique instances of them for it to be valuable in our analysis. We could consider binning hashtag into none and at least 1 hashtag(s), however that wouldn't be useful for our analysis since its rare for tiktok followers to actually look at the hashtags. The same is true for songs; one could consider grouping original songs into one bin and the rest into others. However, from our domain knowledge, its wouldn't be useful to categorize all original songs as similar since most of them could just be user-edited snippets of actual songs.

To address the issues mentioned above, we grouped the data by users and summarized all relevant predictor variables by taking their mean. Our modified dataset now has 8 columns and 254 observations.

```{r}
tiktok_users <- tiktok |>
  dplyr::group_by(user_name)|>
  dplyr::summarize(likes = mean(n_likes),
            shares = mean(n_shares),
            comments = mean(n_comments),
            plays = mean(n_plays),
            followers = mean(n_followers),
            video_length = mean(video_length),
            total_videos = mean(n_total_vids))

head(tiktok_users, 5)
```

Note that no data leakage is introduced in this process since we are just summarizing by the means of the predictor variable per user. When we split, it'll split based on observations, users. We'll now split our data into testing and training.

```{r}
set.seed(29)

tiktok_split <- initial_split(tiktok_users, prop = 0.75)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
tiktok_split
```

Here's a distribution of our response variable, user followers, from our training set.

```{r}
tiktok_train |>
  ggplot(aes(x = followers)) + 
  geom_histogram() + 
  labs(x = "Followers", y = "Count", title = "Distribution of Number of Followers") + 
  scale_x_continuous(labels = label_number()) + 
  theme(aspect.ratio = .5, 
        plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))
```

```{r, echo=FALSE}
min_followers <- min(tiktok_train$followers, na.rm = TRUE)
max_followers <- max(tiktok_train$followers, na.rm = TRUE)
mean_followers <- mean(tiktok_train$followers, na.rm = TRUE)
sd_followers <- sd(tiktok_train$followers, na.rm = TRUE)
print(paste("Mean of followers:", mean_followers))
print(paste("Standard deviation of followers:", sd_followers))
print(paste("Minimum number of followers:", min_followers))
print(paste("Maximum number of followers:", max_followers))
```

The distribution of our response variable follows, is unimodal and
heavily right skewed. The mean is 16,220,526.3 and the standard deviation is 7,710,869.8. The minimum is 8,900,000 and the maximum is 52,300,000. Based on our standard deviation, there seems to be a lot of variation in our dataset; and from our plot, there are major outliers.

Here are the distributions for the predictor variables we are interested in:

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos", "video_length")

long_train <- tiktok_train |>
  select(all_of(predictor_vars)) |>
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(long_train, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(x = "Value", y = "Count", title = "Distribution of Predictor Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_continuous(labels = scales::comma)+
  theme(plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))

```

We would like to know if any of our predictor variables violate any conditions. Hence, we have the following residual plots for each of them.

```{r}
create_individual_residual_plot <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("avg", predictor)) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
      plot.title = element_text(size = 8),
      axis.title = element_blank(), 
      axis.ticks = element_blank(), 
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")
individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, tiktok_users))

combined_plots <- reduce(individual_residual_plots, `+`)

final_plot <- combined_plots + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )

```

It's clear that all our predictors variables violates constant variance. There's a clear outward spread for likes, shares, comments, plays and video_length, while an inward spread for total_videos.

For interpretability, it makes sense to process average bin video length into levels, corresponding to "short", "medium" and "long." This also allows us to search for interesting interactions effects video_length might have with other predictors such as likes. Therefore, we'll add step_discretize() into the recipe for video_length.

In order to deal with constant variance for the other terms, we log transformed the rest of the predictor variables.

```{r}
transformed_tiktok_users <- tiktok |>
  dplyr::group_by(user_name) |>
  dplyr::summarize(
    likes = mean(n_likes),
    shares = mean(n_shares),
    comments = mean(n_comments),
    plays = mean(n_plays),
    followers = mean(n_followers),
    video_length = mean(video_length),
    total_videos = mean(n_total_vids)
  ) |>
  mutate(
    followers = log(followers),
  ) |>
  select(-user_name)

set.seed(29)

tiktok_split <- initial_split(transformed_tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

```{r}
tiktok_recipe <- recipe(followers ~ ., data = tiktok_train) |>
  step_center(all_numeric_predictors()) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors(), -all_outcomes())

prepped_tiktok_recipe <- prep(tiktok_recipe, training = tiktok_train)
train_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_train)
test_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_test)
```

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")  

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, train_transformed))

combined_residual_plot <- wrap_plots(individual_residual_plots, ncol = 2)

final_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

The residual plots don't seem to suggest any underlying patterns about linearity. As such, we conclude the predictors satisfy linearity and constant variance.

We can also assume independence is met. Each of the videos are by individual creators, therefore the videos are independent of each other.

We continue with normality.

```{r}
create_individual_residual_histogram <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .resid)) +
    geom_histogram(bins = 40, fill = "blue", color = "black", alpha = 0.5) +
    labs(title = paste(predictor)) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
      plot.title = element_text(size = 12, face = "bold"),
      axis.title.x = element_blank(), 
      axis.title.y = element_blank(), 
      axis.ticks = element_blank(),
      axis.text.y = element_text(size = 8)
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")

individual_residual_histograms <- map(predictor_vars, ~create_individual_residual_histogram(.x, tiktok_users))

combined_residual_plot <- wrap_plots(individual_residual_histograms, ncol = 2)

final_hist_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residual Distribution for Each Predictor",
    caption = "Residual",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_hist_plot <- final_hist_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_hist_plot) +
  labs(tag = "count") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

Normality seems to be satisfied for each of the predictors except total_videos and possibly shares. However, even though total_videos and shares do not have completely normal distributions, because we have more than 30 observations in the dataset, we can conclude that normality is satisfied regardless of the distribution.

#### **Methodology**

Before constructing our model, we chose to log transformed all our variables in the dataset 'transformed_tiktok_users' because, prior to the transformation, the variables were not to scale and returned coefficients of 0.000. 

Afterwards, we constructed two linear regression model fitting variable 'followers' with predictor variables 'likes', 'shares', 'comments', 'plays', 'video_length_bin', and 'total_videos.' Our first model m1 had all the previously indicated predictor variables excluding variable 'likes' while our second model m2 had those predictor variables excluding variable 'plays.' We compared these two models because likes and plays had the highest vif values in our VIF test (14.431 and 12.253 respectively), meaning they have the highest likelihood for multicollinearity. We chose the model without plays, m2, because it had a lower AIC and BIC value, indicating that it was a better fit. Therefore, we choose to remove plays from the model and leave likes in the model to deal with the multicollinearity. 

In our recipe, we fit the dataset 'tiktok_user,' after which we added steps omitting all NA values from our predictors and log-transforming all our predictor variables and followers. We then conducted a cross-validation test on tiktok_train.

#### Detecting Multicollinearity & Model Comparison

```{r}
tiktok_users_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ likes + shares + comments + plays + video_length_bin2 + video_length_bin3 + total_videos, data=train_transformed)

vif(tiktok_users_fit$fit)
```

```{r}
m1 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - likes, data = train_transformed)
  
m1 |>
  tidy() |>
  kable(digits = 3)
```

```{r}
m2 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - plays, data = train_transformed)
  
m2 |>
  tidy() |>
  kable(digits = 3)
```

**Model Comparison with 5-fold CV**

```{r}
set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)

calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}

tiktok_recipe1 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(likes) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_recipe2 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(plays) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_wflow1 <- workflow() |>
  add_recipe(tiktok_recipe1) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_wflow2 <- workflow() |>
  add_recipe(tiktok_recipe2) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs1 <- tiktok_wflow1 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

tiktok_fit_rs2 <- tiktok_wflow2 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

Model 1: (without likes):

RMSE:

```{r}

collect_metrics(tiktok_fit_rs1, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean)

```

Adj. r-sq, AIC, BIC:

```{r}
map_df(tiktok_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

Model 2 (without plays):

RMSE:

```{r}
collect_metrics(tiktok_fit_rs2, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean)
```

Adj. r-sq, AIC, BIC:

```{r}
map_df(tiktok_fit_rs2$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

While model 1 has a higher RMSE, it also has a lower AIC and BIC, and a higher adjusted r-squared. Because of this, we can conclude that model 1 (the model without likes) is a better fit. Therefore, we choose to remove likes from the model and leave plays in the model to deal with the multicollinearity.

### Determining whether video_length_bin are necessary

We can see from our previous tidy table that the p-values associated with the video length bins are high, indicating that the variables aren't significant. Because of this, we can do a cross validation test to see how a model without video_length performs

RMSE:

```{r}

set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)


tiktok_recipe3 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(likes) |>
  step_rm(video_length) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_wflow3 <- workflow() |>
  add_recipe(tiktok_recipe3) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs3 <- tiktok_wflow3 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

collect_metrics(tiktok_fit_rs3, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean)

```

Adj. r-sq, AIC, BIC:

```{r}
map_df(tiktok_fit_rs3$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

We can see that when we remove video_length, AIC and BIC both significantly decrease and adjusted r-squared increases. However, RMSE also increases. This means that a model without video_length may fit our data better, however it also performs worse when it comes to prediction (there is more distance between predicted and observed values). Because of this, we should keep video_length in our model.

### Determining whether interaction terms are needed

We also want to determine whether there are any significant interaction effects among our predictor variables. To do this, we can start by adding all of the interaction terms associated with video_length bins and observing our tidy table for significant coefficients.

This is a table with all of our interaction terms:

```{r}
model_with_interaction <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ plays + shares + comments + total_videos + video_length_bin2 + video_length_bin3 + video_length_bin2*(likes + shares + total_videos) + video_length_bin3*(likes + shares + total_videos), data = train_transformed)

model_with_interaction |>
  tidy(conf.int = TRUE) |>
  kable(digits = 3)
```

We can see from the p-values in the table that likes:video_length_bin3 and shares:video_length_bin3 seem to be the only statistically significant interaction terms. Therefore, those terms should certainly be in our model. All three of the interaction terms associated with video_length_bin2 have extremely high p-values and low coefficients, meaning that video_length_bin2 is insignificant. The p-value for total_videos:video_length_bin3 is not less than our significance level of 0.05, but it is still low. We can now perform two CV tests: one with likes:video_length_bin3 and shares:video_length_bin3, and one with total_videos:video_length_bin3, as well.

Model 1 (with two interaction terms):

RMSE:

```{r}
tiktok_recipe4 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(likes) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_interact(~ likes:video_length) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_wflow4 <- workflow() |>
  add_recipe(tiktok_recipe4) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs4 <- tiktok_wflow4 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

collect_metrics(tiktok_fit_rs3, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean)
```

Given that the mean RMSE (0.3389576) is barely larger than our original model without likes and with video_length_bin (0.3398198), we choose the more parsimonious model and select our original model without likes, with video_length_bin, and no interaction terms.

#### Results

```{r}

predict_test <- predict(m1, new_data = test_transformed) |>
  bind_cols(tiktok_test) 
  
#print(colnames(predict_test))
#str(predict_test)

predict_test <- predict_test |>
  mutate(
    .pred = as.numeric(.pred),
    followers = as.numeric(followers)
  )|>
  mutate(
    .pred = round(.pred, digits=0),
    followers = round(followers, digits=0)
  )

rmse_result <- rmse(predict_test, truth = followers, estimate = .pred)
rsquared <- rsq(predict_test, truth = followers, estimate = .pred)
rmse_result
rsquared
```

RMSE of 1.867 (exponentially scaled) and RSQ of 0.205 indicates that the model does poorly in predicting the number of followers. Only about 20.5% of the variability in followers can be explained by our predictor variables. On average, the estimated follower count is about 1.867 followers from the observed values.

```{r}
predict_test|>
  select(c(.pred, followers))
```

```{r}
original_scale_rmse <- exp(rmse_result$.estimate)
original_scale_rmse
```

```{r}
m1 |>
  tidy() |>
  kable(digits = 3)
```

We can see from this model above that there are several terms that are significant when determining the number of followers a tik tok user has. Likes, for example, always had the strongest correlation with followers throughout our modeling process. This makes sense, as likes represent how much the users are enjoying a creator's content. Total videos, as well, seems to have a clear positive relationship with follower count. This also would align with our expectations, as the more videos you make, the more engagement your profile is likely to have. Lastly, we can look at the video length bin variable, which separates a user's average video length into three bins. We can see from the model, that the middle video length bin (2) has statistically significant difference from the other two video length bins, as well as a statistically significant interaction term with total videos. This shows that not only do medium length videos generate the most followers, but medium length videos combined with a higher number of total videos significantly increase follower count as well. This is certainly an interesting finding from our analysis, as it isn't the most expected result.

#### Conclusion

We originally decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account. We learned that it is extremely difficult to correctly predict follower count, given our model only captures 20.5% of the variability in the dataset. More complex models seemed to only worsen performance, and we chose to prioritize parsimony for this reason - however, even the simple models did not predict well.

Our dataset was extremely difficult to work with, given that it did not meet the conditions for linear regression (linearity and constant variance), and contained multicollinearity. The variables were also extremely large, and needed to be scaled down to have meaningful coefficients - which made late interpretation significantly more difficult. A more complex model was likely needed, that was beyond the scope of our knowledge, given how poorly our model performed at the end. There also may be underlying relationships between follower count, and other portions of the TikTok algorithm that are not contained in the dataset, which our model might have also failed to capture. In order to improve our analysis, it would be helpful to comb TikTok for a dataset that potentially contains more variables.

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::
