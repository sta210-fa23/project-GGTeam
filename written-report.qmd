---
title: "Predicting Tik-Tok User Data Based on Video Data"
author: "GGteam: Will Chen, Katelyn Cai, Hannah Choi, Weston Slayton"
date: "Nov 9"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data
#| warning: false
#| message: false
library(dplyr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(car)
library(knitr)
library(yardstick)
library(broom)
library(recipes)
```

```{r}
tiktok <- read.csv("data/top_users_vids.csv")
```

#### Introduction and data

TikTok now has over 1 billion users globally, making it one of the fastest growing social platforms in the world. As it has risen to prominence, so has its ubiquitous algorithm, which is said to generally account for account factors (likes and comments) and video information (captions, sounds, hashtags). Given, that TikTok has been heavily criticized alongside other platforms for declining youth mental health outcomes and rising hate due to the addictive nature of its explore page, we decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account, like average number of videos, average number of likes, and average number of comments. 

The dataset comes from the 'top_users_vids.csv' file (under folder 'Trending Videos Data Collection') of the Github repository found at: https://github.com/ivantran96/TikTok_famous/tree/main. The data was originally collected as part of the DataResolutions's Data Blog project exploring Tiktok's demographics and trending video analytics. 

The original data curators collected the data using David Teather's open-source Unofficial Tiktok API (found at https://github.com/davidteather/TikTok-Api), which uses Python to scrape Tiktok data and fetch the most trending videos, specific user information, and much more. Using the list of top Tiktokers, the curators compiled a list of users with the getSuggestedUsersbyIDCrawler api method, which used the top TikTokers and collected the suggested users. Using the byUsername method, they collected video data of the 25 most recent posts of each user from the top TikTokers and the suggested list. The curators also used the API's bySound method to collect videos using some of the most famous songs on TikTok to get an idea of how the choice of music can impact the potential of a video to become a trending video.

#### **EDA**

We begin our EDA process by first examining the dataset.

```{r}
print(head(tiktok), digits = 2, width = 80)
```

We have the following columns:

```{r}
names(tiktok)
```

Currently, our dataset tiktok has 13 columns and 12,559 observations. The columns cover attributes of a tiktok video such as video length, hashtags used, songs/sounds used, and number of likes, shares, comments, plays, and followers (and their total number of likes and videos). Variables id, create_time, video_length, n_likes, n_shares, n_comments, n_plays, n_followers, n_total_likes, and n_total_vids are numerical while the others are categorical.

However, from just our initial exploration, it's clear that some of the columns won't be useful for our analysis. It is also apparent that we should find a way to address the potential issue user_name might have with the other columns. There's a potential for severe multicollinearity if we choose to just drop user_name, since the number of plays or likes a video would have a strong relationship with the user that post it. Therefore, any analysis without user and its related features would have to consider the user's account as confounding variables. In addition, we'll be forced to drop valuable features directly related to a user such as user followers, user total likes and user total videos (n_followers, n_total_likes, n_total_vids).

We see that the less relevant variables are create time and video ID. In addition, from looking at our data, hash tags and songs might not be useful. Most videos don't include a hashtag and there are too many unique instances of them for it to be valuable in our analysis. We could consider binning hashtag into none and at least 1 hashtag(s), however that wouldn't be useful for our analysis since its rare for tiktok followers to actually look at the hashtags. The same is true for songs; one could consider grouping original songs into one bin and the rest into others. However, from our domain knowledge, its wouldn't be useful to categorize all original songs as similar since most of them could just be user-edited snippets of actual songs.

To address the issues mentioned above, we grouped the data by users and summarized all relevant predictor variables by taking their mean. Our modified dataset now has 8 columns and 254 observations.

```{r}
tiktok_users <- tiktok |>
  dplyr::group_by(user_name)|>
  dplyr::summarize(likes = mean(n_likes),
            shares = mean(n_shares),
            comments = mean(n_comments),
            plays = mean(n_plays),
            followers = mean(n_followers),
            video_length = mean(video_length),
            total_videos = mean(n_total_vids))

head(tiktok_users, 10)
```

Note that no data leakage is introduced in this process since we are just summarizing by the means of the predictor variable per user. When we split, it'll split based on observations, users. We'll now split our data into testing and training.

```{r}
set.seed(29)

tiktok_split <- initial_split(tiktok_users, prop = 0.75)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)

tiktok_split
```

Here's a distribution of our response variable, user followers, from our training set.

```{r}
tiktok_train |>
  ggplot(aes(x = followers)) + 
  geom_histogram() + 
  labs(x = "Followers", y = "Count", title = "Distribution of Number of Followers") + 
  scale_x_continuous(labels = label_number())
```

Here are the distributions for the predictor variables we are interested in:

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos", "video_length")

long_train <- tiktok_train |>
  select(all_of(predictor_vars)) |>
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(long_train, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(x = "Value", y = "Count", title = "Distribution of Predictor Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_continuous(labels = scales::comma)

```

We would like to know if any of our predictor variables violate any conditions. Hence, we have the following residual plots for each of them.

```{r}
create_individual_residual_plot <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("avg", predictor)) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
      plot.title = element_text(size = 10, face = "bold"),
      axis.title = element_blank(), 
      axis.ticks = element_blank(), 
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")
individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, tiktok_users))

combined_plots <- reduce(individual_residual_plots, `+`)

final_plot <- combined_plots + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 11)
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 10),
    plot.subtitle = element_text(size = 15)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )

```

It's clear that all our predictors variables violates constant variance. There's a clear outward spread for likes, shares, comments, plays and video_length, while an inward spread for total_videos.

For interpretability, it makes sense to process average bin video length into levels, corresponding to "short", "medium" and "long." This also allows us to search for interesting interactions effects video_length might have with other predictors such as likes. Therefore, we'll add step_discretize() into the recipe for video_length.

In order to deal with constant variance for the other terms, we log transformed the rest of the predictor variables.

```{r}
transformed_tiktok_users <- tiktok |>
  dplyr::group_by(user_name) |>
  dplyr::summarize(
    likes = mean(n_likes),
    shares = mean(n_shares),
    comments = mean(n_comments),
    plays = mean(n_plays),
    followers = mean(n_followers),
    video_length = mean(video_length),
    total_videos = mean(n_total_vids)
  ) |>
  mutate(
    followers = log(followers + 1),
  ) |>
  select(-user_name)

set.seed(29)

tiktok_split <- initial_split(transformed_tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

```{r}
tiktok_recipe <- recipe(followers ~ ., data = tiktok_train) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors()) |>
  step_center(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors(), -all_outcomes())

prepped_tiktok_recipe <- prep(tiktok_recipe, training = tiktok_train)
train_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_train)
test_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_test)
```

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")  

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, train_transformed))

combined_residual_plot <- wrap_plots(individual_residual_plots, ncol = 2)

final_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 11)
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 10),
    plot.subtitle = element_text(size = 15)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )
```

The residual plots don't seem to suggest any underlying patterns about linearity. As such, we conclude the predictors satisfy linearity and constant variance.

We can also assume independence is met. Each of the videos are by individual creators, therefore the videos are independent of each other.

We continue with normality.

```{r}
create_individual_residual_histogram <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .resid)) +
    geom_histogram(bins = 40, fill = "blue", color = "black", alpha = 0.5) +
    labs(title = paste(predictor)) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
      plot.title = element_text(size = 12, face = "bold"),
      axis.title.x = element_blank(), 
      axis.title.y = element_blank(), 
      axis.ticks = element_blank(),
      axis.text.y = element_text(size = 8)
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")

individual_residual_histograms <- map(predictor_vars, ~create_individual_residual_histogram(.x, tiktok_users))

combined_residual_plot <- wrap_plots(individual_residual_histograms, ncol = 2)

final_hist_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residual Distribution for Each Predictor",
    caption = "Residual",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 11)
    )
  ) 

final_hist_plot <- final_hist_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 10),
    plot.subtitle = element_text(size = 15)
  )

wrap_elements(final_hist_plot) +
  labs(tag = "count") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )
```

Normality seems to be satisfied for each of the predictors except total_videos and possibly shares.

For now, this concludes our EDA/data cleaning process and we move onto our model selection process.

Methodology

This section includes a brief description of your modeling process. Explain the reasoning for the type of model you're fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.

Before constructing our model, we chose to log transformed all our variables in the dataset 'transformed_tiktok_users' because, prior to the transformation, the variables were not to scale and returned coefficients of 0.000. 

Afterwards, we constructed two linear regression model fitting variable 'followers' with predictor variables 'likes', 'shares', 'comments', 'plays', 'video_length_bin', and 'total_videos.' Our first model m1 had all the previously indicated predictor variables excluding variable 'likes' while our second model m2 had those predictor variables excluding variable 'plays.' We compared these two models because likes and plays had the highest vif values in our VIF test (14.431 and 12.253 respectively), meaning they have the highest likelihood for multicollinearity. We chose the model without plays, m2, because it had a lower AIC and BIC value, indicating that it was a better fit. Therefore, we choose to remove plays from the model and leave likes in the model to deal with the multicollinearity. 

In our recipe, we fit the dataset 'tiktok_user,' after which we added steps omitting all NA values from our predictors and log-transforming all our predictor variables and followers. We then conducted a cross-validation test on tiktok_train.

#### Detecting Multicollinearity & Model Comparison

```{r}
tiktok_users_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ likes + shares + comments + plays + video_length_bin2 + video_length_bin3 + total_videos, data=train_transformed)

vif(tiktok_users_fit$fit)
```

```{r}
m1 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - likes, data = train_transformed)
  
m1 |>
  tidy() |>
  kable(digits = 3)
```

```{r}
m2 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - plays, data = train_transformed)
  
m2 |>
  tidy() |>
  kable(digits = 3)
```

**Model Comparison with 5-fold CV**

```{r}
set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)

calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}

tiktok_recipe1 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(likes) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_recipe2 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(plays) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_wflow1 <- workflow() |>
  add_recipe(tiktok_recipe1) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_wflow2 <- workflow() |>
  add_recipe(tiktok_recipe2) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs1 <- tiktok_wflow1 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

tiktok_fit_rs2 <- tiktok_wflow2 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

```{r}
collect_metrics(tiktok_fit_rs1, summarize = TRUE)
map_df(tiktok_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

```{r}
collect_metrics(tiktok_fit_rs2, summarize = TRUE)
map_df(tiktok_fit_rs2$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

```{r}
glance(m1)|>
  select(adj.r.squared, AIC, BIC)
```

```{r}
glance(m2)|>
  select(adj.r.squared, AIC, BIC)
```

Based on AIC and BIC, model 2 (the model without plays) is a better fit. Therefore, we choose to remove plays from the model and leave likes in the model to deal with the multicollinearity.

### Determining whether interaction terms are needed

We also want to determine whether there are any significant interaction effects among our predictor variables. To do this, we can start by adding all of the interaction terms we are interested in, and then eliminating the insignificant variables through assessing p-value and cross validation.

This is a table with all of our interaction terms:

```{r}
model_with_interaction <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ likes + shares + comments + total_videos + video_length_bin2 + video_length_bin3 + video_length_bin2*(likes + shares + total_videos) + video_length_bin3*(likes + shares + total_videos), data = train_transformed)

model_with_interaction |>
  tidy(conf.int = TRUE) |>
  kable(digits = 3)
```

We can see from the p-values in the table that likes:video_length_bin3 and shares:video_length_bin3 seem to be the only statistically significant interaction terms. Therefore, those terms should certainly be in our model. All three of the interaction terms associated with video_length_bin2 have extremely high p-values and low coefficients, meaning that video_length_bin2 is insignificant. The p-value for total_videos:video_length_bin3 is not less than our significance level of 0.05, but it is still low. We can use AIC and BIC tests to compare how well a model with our without this term fits our data.

```{r}
model_with_interaction1 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ likes + shares + comments + total_videos + video_length_bin2 + video_length_bin3 + likes * video_length_bin3 + shares * video_length_bin3, data = train_transformed)

model_with_interaction2 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ likes + shares + comments + total_videos + video_length_bin2 + video_length_bin3 + likes * video_length_bin3 + shares * video_length_bin3 + total_videos * video_length_bin3, data = train_transformed)
```

Model 1:

```{r}
glance(model_with_interaction1)|>
  select(adj.r.squared, AIC, BIC)
```

Model2:

```{r}
glance(model_with_interaction2)|>
  select(adj.r.squared, AIC, BIC)
```

While the additional interaction term marginally increases the adjusted r-squared and slightly decreases the AIC, it also increases the BIC by about 3. Because of these mixed results, we chose the simpler model (the model without the additional interaction term), in pursuit of parsimony.

#### Results

In this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.

This section also includes initial interpretations and conclusions drawn from the model.

```{r}
#| eval: FALSE
# REDO
predict_test <- predict(model_with_interaction2, new_data = test_transformed) |>
  bind_cols(tiktok_test) 
  
#print(colnames(predict_test))
#str(predict_test)
predict_test <- predict_test |>
  mutate(
    .pred = as.numeric(.pred),
    followers = as.numeric(followers)
  )

rmse_result <- rmse(predict_test, truth = followers, estimate = .pred)
rsquared <- rsq(predict_test, truth = followers, estimate = .pred)
rmse_result
rsquared
```

RMSE of 0.5345 and Rsq of 0.168 indicates that the model does poorly in predicting the number of followers.

```{r}
predict_test|>
  select(c(.pred, followers))
```

```{r}
#| eval: FALSE
original_scale_rmse <- exp(rmse_result$.estimate)
original_scale_rmse
```

```{r}
#| eval: FALSE
model_with_interaction2 |>
  tidy() |>
  kable(digits = 3)
```

We can see from this model above that there are several terms that are significant when determining the number of followers a tik tok user has. Likes, for example, always had the strongest correlation with followers throughout our modeling process. This makes sense, as likes represent how much the users are enjoying a creator's content. Total vidoes, as well, seems to have a clear positive relationship with follower count. This also would align with our expectations, as the more videos you make, the more engagement your profile is likely to have. Lastly, we can look at the video length bin variable, which separates a user's average video length into three bins. We can see from the model, that the middle video length bin (2) has statistically significant difference from the other two video length bins, as well as a statistically significant interaction term with total videos. This shows that not only do medium length videos generate the most followers, but medium length videos combined with a higher number of total videos significantly increase follower count as well. This is certainly an interesting finding from our analysis, as it isn't the most expected result.

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::
