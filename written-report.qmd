---
title: "Predicting Tik-Tok User Data Based on Video Data"
author: "GGteam: Will Chen, Katelyn Cai, Hannah Choi, Weston Slayton"
date: "Nov 9"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data
#| warning: false
#| message: false
library(dplyr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(car)
library(knitr)
```

Below is a brief description of the sections to focus on in the draft:

```{r}
tiktok <- read.csv("data/top_users_vids.csv")
```

#### Introduction and data

TikTok now has over 1 billion users globally, making it one of the fastest growing social platforms in the world. As it has risen to prominence, so has its ubiquitous algorithm, which is said to generally account for account factors (likes and comments) and video information (captions, sounds, hashtags). Given, that TikTok has been heavily criticized alongside other platforms for declining youth mental health outcomes and rising hate due to the addictive nature of its explore page, we decided to look at TikTok\'s data and how follower count (a huge driver of engagement) is impacted by other aspects of a user\'s account, like average number of videos, average number of likes, and average number of comments. 

The dataset comes from the \'top_users_vids.csv\' file (under folder \'Trending Videos Data Collection\') of the Github repository found at: https://github.com/ivantran96/TikTok_famous/tree/main. The data was originally collected as part of the DataResolutions\'s Data Blog project exploring Tiktok\'s demographics and trending video analytics. 

The original data curators collected the data using David Teather\'s open-source Unofficial Tiktok API (found at https://github.com/davidteather/TikTok-Api), which uses Python to scrape Tiktok data and fetch the most trending videos, specific user information, and much more. Using the list of top Tiktokers, the curators compiled a list of users with the getSuggestedUsersbyIDCrawler api method, which used the top TikTokers and collected the suggested users. Using the byUsername method, they collected video data of the 25 most recent posts of each user from the top TikTokers and the suggested list. The curators also used the API\'s bySound method to collect videos using some of the most famous songs on TikTok to get an idea of how the choice of music can impact the potential of a video to become a trending video.

The original dataset tiktok has 13 columns and 12,559 observations. The columns cover important metrics for trending videos such as video length, hashtags used, songs/sounds used, and number of likes, shares, comments, plays, and followers (and their total number of likes and videos). There are also less relevant variables such as creator username, create time, video ID which we will not use in our analysis. Variables id, create_time, video_length, n_likes, n_shares, n_comments, n_plays, n_followers, n_total_likes, and n_total_vids are numerical while the others are categorical.

For EDA, we grouped the data by users and summarized all relevant predictor variables by taking their mean. Our modified dataset now has 8 columns and 254 observations. Username refers to the username of the person the other metrics refer to, which is drawn from the list of top TikTokers. Likes is the average number of likes across the most recent 25 posts of 

We decided to group this data by user to limit multicollinearity between variables when looking at a specific video, because views would be very directly correlated with likes and comments; whereas, when looking at a user as a whole and averaging variables across videos, that chance of multicollinearity decreases. 

```{r}
tiktok_users <- tiktok|>
  dplyr::group_by(user_name)|>
  dplyr::summarize(likes = mean(n_likes),
            shares = mean(n_shares),
            comments = mean(n_comments),
            plays = mean(n_plays),
            followers = mean(n_followers),
            video_length = mean(video_length),
            total_videos = mean(n_total_vids)) |>
  mutate(video_length_bin = ntile(video_length, n=3)) |>
  mutate(video_length_bin = as.factor(video_length_bin))
```

```{r}
tiktok_users |>
  ggplot(aes(x = followers)) + 
  geom_histogram() + 
  labs(x = "Followers", y = "Count", title = "Distribution of Number of Followers") + 
  scale_x_continuous(labels = label_number())
```

Since our data is heavily skewed, we continue our analysis by examining potential.

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "video_length", "total_videos")

create_individual_residual_plot <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes_string(x = ".fitted", y = ".resid")) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted Values", y = "Residuals", title = paste("Residuals vs Fitted for", predictor)) +
    theme_minimal()
}

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, tiktok_users))
# from individual plots scaled to size, it's clear that constant variance is not met
# patchwork for visualization 
combined_plots <- reduce(individual_residual_plots, `+`)
final_plot <- combined_plots + plot_layout(ncol = 3, nrow = 2)
final_plot
```

```{r}
transformed_tiktok_users <- tiktok |>
  dplyr::group_by(user_name) |>
  dplyr::summarize(
    likes = mean(n_likes),
    shares = mean(n_shares),
    comments = mean(n_comments),
    plays = mean(n_plays),
    followers = mean(n_followers),
    video_length = mean(video_length),
    total_videos = mean(n_total_vids)
  ) |>
  mutate(video_length_bin = ntile(video_length, n = 3)) |>
  mutate(
    followers = log(followers + 1),
    shares = log(shares + 1),
    comments = log(comments + 1),
    plays = log(plays + 1),
    likes = log(likes + 1),
    total_videos = log(total_videos + 1)
  ) |>
  mutate_at(vars(followers, shares, comments, plays, likes, total_videos),
            scale)
```

#### Methodology

This section includes a brief description of your modeling process. Explain the reasoning for the type of model you're fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.

#### Detecting Multicollinearity & Model Comparison

```{r}
tiktok_users_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ likes + shares + comments + plays + video_length_bin + total_videos, data=transformed_tiktok_users)
vif(tiktok_users_fit$fit)
```

```{r}
m1 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - likes - video_length- user_name, data = transformed_tiktok_users)
  
m1 |>
  tidy() |>
  kable(digits = 3)
```

```{r}
m2 <- linear_reg() |>
  set_engine("lm") |>
  fit(log(followers) ~ . - plays - video_length - user_name, data = transformed_tiktok_users)
  
m2 |>
  tidy() |>
  kable(digits = 3)
```

```{r}
glance(m1)|>
  select(adj.r.squared, AIC, BIC)
```

```{r}
glance(m2)|>
  select(adj.r.squared, AIC, BIC)
```

Based on AIC and BIC, model 2 (the model without plays) is a better fit. Therefore, we choose to remove plays from the model and leave likes in the model to deal with the multicollinearity.

```{r}
tiktok_aug <- augment(m2$fit)

ggplot(data = tiktok_aug, aes(x = .fitted, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

resid_hist <- ggplot(data = tiktok_aug, aes(x = .resid)) +
    geom_histogram() +
    labs(x = "Residuals",
         title = "Distribution of residuals")
  resid_hist
```

```{r}
ggplot(data = tiktok_aug, aes(x = likes, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = shares, y = .resid)) +
    geom_point() +
  coord_cartesian(xlim = c(0, 50)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = comments, y = .resid)) +
    geom_point() +
    coord_cartesian(xlim = c(0, 50)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = plays, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = total_videos, y = .resid)) +
    geom_point() +
  coord_cartesian(xlim = c(0, 50)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")
```

### Determining whether interaction terms are needed

```{r}
model_with_interaction <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ likes + shares + comments + total_videos + video_length_bin)
```

#### Results

In this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.

This section also includes initial interpretations and conclusions drawn from the model.

### Cross-Validation

```{r}
set.seed(29)

tiktok_split <- initial_split(tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

```{r}
tiktok_rec <- recipe(followers ~ likes + shares + comments + total_videos + video_length_bin, 
                    data = tiktok_train) |>
  step_naomit(all_predictors())|>
  step_log(all_predictors()) |>
  step_log(followers) 

tiktok_spec <- linear_reg() |>
  set_engine("lm")

tiktok_wflow1 <- workflow() |>
  add_model(tiktok_spec) |>
  add_recipe(tiktok_rec)
```

```{r}

folds <- vfold_cv(tiktok_train, v = 10)

tiktok_cv1 <- tiktok_wflow1 |>
  fit_resamples(resamples = folds) 

collect_metrics(tiktok_cv1, summarize = TRUE)
```

```{r}

```

Your written report goes here!

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::
