---
title: "Predicting Tik-Tok User Data Based on Video Data"
author: "GGteam: Will Chen, Katelyn Cai, Hannah Choi, Weston Slayton"
date: "Nov 9"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data
#| warning: false
#| message: false
library(dplyr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(car)
library(knitr)
library(yardstick)
library(broom)
library(recipes)
```

Below is a brief description of the sections to focus on in the draft:

```{r}
tiktok <- read.csv("data/top_users_vids.csv")
```

#### Introduction and data

TikTok now has over 1 billion users globally, making it one of the fastest growing social platforms in the world. As it has risen to prominence, so has its ubiquitous algorithm, which is said to generally account for account factors (likes and comments) and video information (captions, sounds, hashtags). Given, that TikTok has been heavily criticized alongside other platforms for declining youth mental health outcomes and rising hate due to the addictive nature of its explore page, we decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account, like average number of videos, average number of likes, and average number of comments. 

The dataset comes from the 'top_users_vids.csv' file (under folder 'Trending Videos Data Collection') of the Github repository found at: https://github.com/ivantran96/TikTok_famous/tree/main. The data was originally collected as part of the DataResolutions's Data Blog project exploring Tiktok's demographics and trending video analytics. 

The original data curators collected the data using David Teather's open-source Unofficial Tiktok API (found at https://github.com/davidteather/TikTok-Api), which uses Python to scrape Tiktok data and fetch the most trending videos, specific user information, and much more. Using the list of top Tiktokers, the curators compiled a list of users with the getSuggestedUsersbyIDCrawler api method, which used the top TikTokers and collected the suggested users. Using the byUsername method, they collected video data of the 25 most recent posts of each user from the top TikTokers and the suggested list. The curators also used the API's bySound method to collect videos using some of the most famous songs on TikTok to get an idea of how the choice of music can impact the potential of a video to become a trending video.

The original dataset tiktok has 13 columns and 12,559 observations. The columns cover important metrics for trending videos such as video length, hashtags used, songs/sounds used, and number of likes, shares, comments, plays, and followers (and their total number of likes and videos). There are also less relevant variables such as creator username, create time, video ID which we will not use in our analysis. Variables id, create_time, video_length, n_likes, n_shares, n_comments, n_plays, n_followers, n_total_likes, and n_total_vids are numerical while the others are categorical.

For EDA, we grouped the data by users and summarized all relevant predictor variables by taking their mean. Our modified dataset now has 8 columns and 254 observations. Username refers to the username of the person the other metrics refer to, which is drawn from the list of top TikTokers. Likes is the average number of likes across the most recent 25 posts of 

We decided to group this data by user to limit multicollinearity between variables when looking at a specific video, because views would be very directly correlated with likes and comments; whereas, when looking at a user as a whole and averaging variables across videos, that chance of multicollinearity decreases. 

```{r}
tiktok_users <- tiktok |>
  dplyr::group_by(user_name)|>
  dplyr::summarize(likes = mean(n_likes),
            shares = mean(n_shares),
            comments = mean(n_comments),
            plays = mean(n_plays),
            followers = mean(n_followers),
            video_length = mean(video_length),
            total_videos = mean(n_total_vids))
```

```{r}
set.seed(29)

tiktok_split <- initial_split(tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

Here's a distribution of our response variable, user followers, from our training set.

```{r}
tiktok_train |>
  ggplot(aes(x = followers)) + 
  geom_histogram() + 
  labs(x = "Followers", y = "Count", title = "Distribution of Number of Followers") + 
  scale_x_continuous(labels = label_number())
```

We would like to know if any of our predictor variables violate any conditions. Hence, we have the following residual plots for each of them.

```{r}
create_individual_residual_plot <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted Values", y = "Residuals", title = paste("avg", predictor)) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
      plot.title = element_text(size = 14, face = "bold"),
      axis.title = element_text(size = 12)
    )
}

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, tiktok_users))

# Patchwork for visualization 
combined_plots <- reduce(individual_residual_plots, `+`)
final_plot <- combined_plots + plot_layout(ncol = 3, nrow = 2)
final_plot
```

It's clear that all our predictors variables violates constant variance. There's a clear outward spread for likes, shares, comments, plays and video_length, while an inward spread for total_videos.

For interpretability, it makes sense to process average bin video length into levels, corresponding to "short", "medium" and "long." This also allows us to search for interesting interactions effects video_length might have with other predictors such as likes. Therefore, we'll add step_discretize() into the recipe for video_length.

In order to deal with constant variance for the other terms, we log transformed the rest of the predictor variables.

```{r}
transformed_tiktok_users <- tiktok |>
  dplyr::group_by(user_name) |>
  dplyr::summarize(
    likes = mean(n_likes),
    shares = mean(n_shares),
    comments = mean(n_comments),
    plays = mean(n_plays),
    followers = mean(n_followers),
    video_length = mean(video_length),
    total_videos = mean(n_total_vids)
  ) |>
  mutate(
    followers = log(followers + 1),
    shares = log(shares + 1),
    comments = log(comments + 1),
    plays = log(plays + 1),
    likes = log(likes + 1),
    total_videos = log(total_videos + 1)
  ) |>
  select(-user_name)

set.seed(29)

tiktok_split <- initial_split(transformed_tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

```{r}
tiktok_recipe <- recipe(followers ~ ., data = tiktok_train) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors()) |>
  step_center(all_numeric_predictors())

prepped_tiktok_recipe <- prep(tiktok_recipe, training = tiktok_train)
train_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_train)
```

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")  

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, train_transformed))

combined_residual_plot <- wrap_plots(individual_residual_plots, ncol = 2)

combined_residual_plot
```

Although there's still signs of fanning going on, it's a lot less severe than before. For now, this concludes our EDA/data cleaning process and we move onto our model selection process.

#### Methodology

This section includes a brief description of your modeling process. Explain the reasoning for the type of model you're fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.

Before constructing our model, we chose to log transformed all our variables in the dataset 'transformed_tiktok_users' because, prior to the transformation, the variables were not to scale and returned coefficients of 0.000. 

Afterwards, we constructed two linear regression model fitting variable 'followers' with predictor variables 'likes', 'shares', 'comments', 'plays', 'video_length_bin', and 'total_videos.' Our first model m1 had all the previously indicated predictor variables excluding variable 'likes' while our second model m2 had those predictor variables excluding variable 'plays.' We compared these two models because likes and plays had the highest vif values in our VIF test (14.431 and 12.253 respectively), meaning they have the highest likelihood for multicollinearity. We chose the model without plays, m2, because it had a lower AIC and BIC value, indicating that it was a better fit. Therefore, we choose to remove plays from the model and leave likes in the model to deal with the multicollinearity. 

In our recipe, we fit the dataset 'tiktok_user,' after which we added steps omitting all NA values from our predictors and log-transforming all our predictor variables and followers. We then conducted a cross-validation test on tiktok_train.

#### Detecting Multicollinearity & Model Comparison

```{r}
tiktok_users_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ likes + shares + comments + plays + video_length_bin2 + video_length_bin3 + total_videos, data=train_transformed)

vif(tiktok_users_fit$fit)
```

```{r}
m1 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - likes, data = train_transformed)
  
m1 |>
  tidy() |>
  kable(digits = 3)
```

```{r}
m2 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ . - plays, data = train_transformed)
  
m2 |>
  tidy() |>
  kable(digits = 3)
```

**Model Comparison with 5-fold CV**

```{r}
set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)

calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}

tiktok_recipe1 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(likes) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_recipe2 <- recipe(followers ~ ., data = tiktok_train) |>
  step_rm(plays) |>
  step_discretize(video_length, options = list(cuts = 3)) |>
  step_dummy(all_nominal_predictors(), -all_outcomes()) |>
  step_center(all_numeric_predictors(), -all_outcomes())

tiktok_wflow1 <- workflow() |>
  add_recipe(tiktok_recipe1) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_wflow2 <- workflow() |>
  add_recipe(tiktok_recipe2) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs1 <- tiktok_wflow1 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

tiktok_fit_rs2 <- tiktok_wflow2 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

```{r}
collect_metrics(tiktok_fit_rs1, summarize = TRUE)
map_df(tiktok_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

```{r}
collect_metrics(tiktok_fit_rs2, summarize = TRUE)
map_df(tiktok_fit_rs2$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

```{r}
glance(m1)|>
  select(adj.r.squared, AIC, BIC)
```

```{r}
glance(m2)|>
  select(adj.r.squared, AIC, BIC)
```

Based on AIC and BIC, model 2 (the model without plays) is a better fit. Therefore, we choose to remove plays from the model and leave likes in the model to deal with the multicollinearity.

```{r}
tiktok_aug <- augment(m2$fit)

ggplot(data = tiktok_aug, aes(x = .fitted, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

resid_hist <- ggplot(data = tiktok_aug, aes(x = .resid)) +
    geom_histogram() +
    labs(x = "Residuals",
         title = "Distribution of residuals")
  resid_hist
```

```{r}
ggplot(data = tiktok_aug, aes(x = likes, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = shares, y = .resid)) +
    geom_point() +
  coord_cartesian(xlim = c(0, 50)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = comments, y = .resid)) +
    geom_point() +
    coord_cartesian(xlim = c(0, 50)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = plays, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")

ggplot(data = tiktok_aug, aes(x = total_videos, y = .resid)) +
    geom_point() +
  coord_cartesian(xlim = c(0, 50)) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(x = "Fitted values",
         y = "Residuals",
         title = "Residuals vs. fitted")
```

### Determining whether interaction terms are needed

```{r}
model_with_interaction <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ likes + shares + comments + total_videos + video_length_bin + likes * video_length_bin + shares * video_length_bin + total_videos * video_length_bin, data = tiktok_train)

model_with_interaction |>
  tidy() |>
  kable(digits = 3)
```

```{r}
model_with_interaction2 <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~ likes + shares + comments + total_videos + video_length_bin + shares * as.factor(video_length_bin == 2) + total_videos * as.factor(video_length_bin == 2) + total_videos * as.factor(video_length_bin == 3), data = tiktok_train)

model_with_interaction2 |>
  tidy() |>
  kable(digits = 3)
```

```{r}
glance(m2)|>
  select(adj.r.squared, AIC, BIC)
```

```{r}
glance(model_with_interaction2)|>
  select(adj.r.squared, AIC, BIC)
```

#### Results

In this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.

This section also includes initial interpretations and conclusions drawn from the model.

```{r}
predict_test <- predict(m2, new_data = tiktok_test) |>
  bind_cols(tiktok_test) 
  
#print(colnames(predict_test))
#str(predict_test)
predict_test <- predict_test |>
  mutate(
    .pred = as.numeric(.pred),
    followers = as.numeric(followers)
  )

rmse_result <- rmse(predict_test, truth = followers, estimate = .pred)
rsquared <- rsq(predict_test, truth = followers, estimate = .pred)
rmse_result
rsquared
```

RMSE of 1.399284 indicates the average prediction error in terms of the log.

```{r}
original_scale_rmse <- exp(rmse_result$.estimate)
original_scale_rmse
```

```{r}

model_with_interaction2 |>
  tidy() |>
  kable(digits = 3)
```

We can see from this model above that there are several terms that are significant when determining the number of followers a tik tok user has. Likes, for example, always had the strongest correlation with followers throughout our modeling process. This makes sense, as likes represent how much the users are enjoying a creator's content. Total vidoes, as well, seems to have a clear positive relationship with follower count. This also would align with our expectations, as the more videos you make, the more engagement your profile is likely to have. Lastly, we can look at the video length bin variable, which separates a user's average video length into three bins. We can see from the model, that the middle video length bin (2) has statistically significant difference from the other two video length bins, as well as a statistically significant interaction term with total videos. This shows that not only do medium length videos generate the most followers, but medium length videos combined with a higher number of total videos significantly increase follower count as well. This is certainly an interesting finding from our analysis, as it isn't the most expected result.

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::
