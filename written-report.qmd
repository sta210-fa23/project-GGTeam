---
title: "Predicting Tik-Tok User Data Based on Video Data"
author: "GGteam: Will Chen, Katelyn Cai, Hannah Choi, Weston Slayton"
date: "12/1/23"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load packages and data
#| warning: false
#| message: false
library(dplyr)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(car)
library(knitr)
library(yardstick)
library(broom)
library(recipes)
```

```{r}
tiktok <- read.csv("data/top_users_vids.csv")
```

#### Introduction and data

With over 1 billion users globally, TikTok is one of the fastest growing social platforms in the world. Understanding ubiquitous algorithm, which is said to generally account for account factors (likes and comments) and video information (captions, sounds, hashtags), is critical to understanding the app's many critiques, from declining youth mental health outcomes and its addictive nature of its explore page. To better understand TikTok's social impact, we decided to explore TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account, like average number of videos, average number of likes, and average number of comments. 

The dataset comes from the 'top_users_vids.csv' file (under folder 'Trending Videos Data Collection') of the Github repository found at: https://github.com/ivantran96/TikTok_famous/tree/main. The data was originally collected as part of the DataResolutions's Data Blog project exploring Tiktok's demographics and trending video analytics. 

The original data curators collected the data using David Teather's open-source Unofficial Tiktok API (found at https://github.com/davidteather/TikTok-Api), which uses Python to scrape Tiktok data and fetch the most trending videos, specific user information, and much more. Using the list of top Tiktokers, the curators expanded the list of users by collecting suggested users with the API's getSuggestedUsersbyIDCrawler method. They then collected video data of the 25 most recent posts of each user using the byUsername method. They also used the bySound method to collect videos using some of the most famous songs on TikTok to get an idea of how the choice of music can impact the potential of a video to start "trending."

#### **EDA**

We begin our EDA process by first examining the dataset.

```{r}
#| include: false
names(tiktok)
```

Currently, our dataset tiktok has 13 columns and 12,559 observations. Each row is a video. The columns cover attributes of each video such as video length, hashtags used, songs/sounds used, and statistics (number of likes, shares, comments, plays, followers, and total number of likes and videos across the account). Variables id, create_time, video_length, n_likes, n_shares, n_comments, n_plays, n_followers, n_total_likes, and n_total_vids are numerical while the others are categorical.

However, it's clear that some of the columns won't be useful for predicting number of followers. It is also apparent that we must address the potential issue user_name might have with the other columns. There's a potential for severe multicollinearity if we choose to just drop user_name, since the number of plays or likes a video would have a strong relationship with the user who posted it. Therefore, any analysis without user and its related features would have to consider the user's account as confounding variables. In addition, we'll be forced to drop valuable features directly related to a user such as user followers, user total likes and user total videos (n_followers, n_total_likes, n_total_vids).

The less relevant variables are create time and video ID. In addition, hashtags and songs might not be useful. Most videos don't include a hashtag and there are too many unique instances of them for it to be valuable in our analysis. We could consider binning hashtag into none and at least 1 hashtag(s), however that wouldn't be useful for our analysis since its rare for tiktok followers to mind the number of hashtags. The same is true for songs; one could consider grouping original songs into one bin and the rest into others. However, from our domain knowledge, its wouldn't be useful to categorize all original songs as similar since most of them could just be user-edited snippets of actual songs.

To address the issues mentioned above, we grouped the data by users and summarized relevant predictor variables by taking their mean. Our modified dataset has 8 columns and 254 observations, with each row being a user.

```{r}
#| include: false
tiktok_users <- tiktok |>
  dplyr::group_by(user_name)|>
  dplyr::summarize(likes = mean(n_likes),
            shares = mean(n_shares),
            comments = mean(n_comments),
            plays = mean(n_plays),
            followers = mean(n_followers),
            video_length = mean(video_length),
            total_videos = mean(n_total_vids))

head(tiktok_users, 5)
```

```{r}
set.seed(29)

tiktok_split <- initial_split(tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

Note that no data leakage is introduced in this process since we are just summarizing by the means of the predictor variable per user. When we split, it'll split based on observations, which are users.

Here's a distribution of our response variable, user followers, from our training set.

```{r}
tiktok_train |>
  ggplot(aes(x = followers/1000000)) + 
  geom_histogram() + 
  labs(x = "Followers (in Millions)", y = "Count", title = "Distribution of Number of 
       Followers", bins=100) +
  scale_x_continuous(labels = label_number()) + 
  theme(aspect.ratio = .5, 
        plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))
```

```{r}
#| echo: FALSE
#| eval: false
min_followers <- min(tiktok_train$followers, na.rm = TRUE)
max_followers <- max(tiktok_train$followers, na.rm = TRUE)
mean_followers <- mean(tiktok_train$followers, na.rm = TRUE)
sd_followers <- sd(tiktok_train$followers, na.rm = TRUE)
print(paste("Mean of followers:", mean_followers))
print(paste("Standard deviation of followers:", sd_followers))
print(paste("Minimum number of followers:", min_followers))
print(paste("Maximum number of followers:", max_followers))
```

The distribution of our response variable follows, is unimodal and heavily right skewed. The mean is 16,220,526.3 and the standard deviation is 7,710,869.8. The minimum is 8,900,000 and the maximum is 52,300,000. Based on our standard deviation, there seems to be a lot of variation in our dataset; and from our plot, we can see major outliers.

Here are the distributions for the predictor variables we are interested in:

```{r}
predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos", "video_length")

long_train <- tiktok_train |>
  select(all_of(predictor_vars)) |>
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(long_train, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(x = "Value", y = "Count", title = "Distribution of Predictor Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_continuous(labels = scales::comma)+
  theme(plot.title = element_text(size = rel(0.8), face = "bold"),
        axis.title.x = element_text(size = rel(0.8)),
        axis.title.y = element_text(size = rel(0.8)))

```

```{r}
transformed_tiktok_users <- tiktok |>
  dplyr::group_by(user_name) |>
  dplyr::summarize(
    likes = mean(n_likes),
    shares = mean(n_shares),
    comments = mean(n_comments),
    plays = mean(n_plays),
    followers = mean(n_followers),
    video_length = mean(video_length),
    total_videos = mean(n_total_vids)
  ) |>
  mutate(
#    followers = log(followers),
 #   comments = comments / 1000,         # Scale comments to hundreds
#    likes = likes / 1000000,            # Scale likes to millions
#    plays = plays / 1000000,            # Scale plays to tens of millions
#    shares = shares / 1000,             # Scale shares to hundreds
#    total_videos = total_videos / 1000  # Scale total_videos to units
    #we scaled comments to hundreds, likes to millions, plays to tens of millions, shares to hundreds, and total_videos to units in order to make the coefficients of these predictors more interpretable
  ) |>
  select(-user_name)

set.seed(29)

tiktok_split <- initial_split(transformed_tiktok_users, prop = 0.7)
tiktok_train <- training(tiktok_split)
tiktok_test  <- testing(tiktok_split)
```

```{r}
tiktok_recipe <- recipe(followers ~ ., data = tiktok_train) |>
  step_center(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_discretize(video_length, options = list(cuts = 3)) 
  
prepped_tiktok_recipe_df <- prep(tiktok_recipe, training = transformed_tiktok_users)
dataset_transformed <- bake(prepped_tiktok_recipe_df, new_data = transformed_tiktok_users)

prepped_tiktok_recipe <- prep(tiktok_recipe, training = tiktok_train)
train_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_train)
test_transformed <- bake(prepped_tiktok_recipe, new_data = tiktok_test)
```

## **Methodology**

We want to use multiple linear regression to predict the number of followers a user has. We choose multiple linear regression rather than logistic regression because followers is a quantiative response variable. We start off with an initial model containing the predictors likes, shares, comments, plays, video_length (factor with 3 levels), total_videos, and followers, our response variable. Because Tiktok videos are commonly divided into 15-second, 1 minute, or 3 minute videos, we bin average video length into 3 levels, corresponding to "short", "medium" and "long." We also mean-center all our numerical variables to make our intercept meaningful. Here is a tidy table of our initial model:

```{r initial-model}

initial_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ ., data=train_transformed)

initial_fit |>
  tidy() |>
  kable(digits = 4)
```

#### Detecting Multicollinearity & Model Comparison

Upon conducting a VIF test, we found that likes and plays had the highest vif values (11.614 and 9.82 respectively):

```{r}
tiktok_users_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ ., data=train_transformed)

vif(tiktok_users_fit$fit) |>
  kable(digits = 3)
```

Therefore, we wanted to assess which model would perform better: a model without likes or a model without plays. To do this, we performed 5-fold cross validation and extracted the resulting AIC, BIC, adj.r-squared, and RMSE values for the two models:

```{r}

set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)

calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}

tiktok_recipe1 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(likes)

tiktok_recipe2 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(plays)

tiktok_wflow1 <- workflow() |>
  add_recipe(tiktok_recipe1) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_wflow2 <- workflow() |>
  add_recipe(tiktok_recipe2) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs1 <- tiktok_wflow1 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

tiktok_fit_rs2 <- tiktok_wflow2 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

Model 1: (without likes):

```{r}
# extract RMSE
rmse <- collect_metrics(tiktok_fit_rs1, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

# extract adjusted R-squared, AIC, and BIC
other_metrics <- map_df(tiktok_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics <- bind_cols(rmse, other_metrics) 
combined_metrics
```

Model 2 (without plays):

```{r}
# do the same for model 2
rmse2 <- collect_metrics(tiktok_fit_rs2, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics2 <- map_df(tiktok_fit_rs2$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics2 <- bind_cols(rmse2, other_metrics2)
combined_metrics2
```

The difference between the model's evaluations aren't large. Model 1 has a higher RMSE, while it has a lower AIC and BIC, and a higher adjusted r-squared. In this case, we would consider model 2 (the model without plays) to be a better model, because it has a lower RMSE, which is gathered from the assessment set and is used to assess prediction. The goal of our model is to predict followers, so we want to choose the model with better predictive power (Model 2). Therefore, we remove plays from our model.

### Determining whether video_length_bin are necessary

We saw from our initial tidy table that the p-values associated with video length bins are high, indicating that the variables may not be significant. Because of this, we can once again perform cross validation to test how a model without video_length compares to our current model (Model 1):

Model 3 (without plays and video length):

```{r}
set.seed(29)
folds <- vfold_cv(tiktok_train, v = 5)
# cv against previously chosen model and model without video length
tiktok_recipe3 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(plays) |>
  step_rm(video_length)

tiktok_wflow3 <- workflow() |>
  add_recipe(tiktok_recipe3) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs3 <- tiktok_wflow3 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )

```

```{r}
rmse3 <- collect_metrics(tiktok_fit_rs3, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics3 <- map_df(tiktok_fit_rs3$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics3 <- bind_cols(rmse3, other_metrics3)
combined_metrics3
```

We can see that when we remove video_length, RMSE is slightly higher than it was for Model 2, while AIC remains about the same and BIC slightly decreases. We also see that adjusted r-squared remains about the same. Therefore, despite BIC slightly decreasing, we prefer the model with a lower RMSE (better prediction), so we don't want to remove video_length from our model.

### Determining whether interaction terms are needed

Our only categorical variable in our model is video_length. Therefore, we can include all possible interaction terms with video_length and assess which combinations look significant:

```{r}

model_with_interaction <- linear_reg() |>
  set_engine("lm") |>
  fit(followers ~  likes + shares + comments + total_videos + video_length + video_length*(likes + shares + comments + total_videos), data = train_transformed)

model_with_interaction |>
  tidy() |>
  kable(digits = 3)
```

We can see from the table that all variables are significant when interacting with video_lengthbin3 (p-value is less than significance level of 0.05) except for total_videos. Because of this, we know that we won't need to include the interaction term between total_videos and video_length. Also, given that comments has a high p-value in this new model, we can try removing comments from our model as well. We can use cross validation to test how a model without comments and with video_length interacting with shares and likes performs compared to our current model (Model 2):

```{r}
#| message: false

tiktok_recipe4 <- recipe(followers ~ ., data = train_transformed) |>
  step_rm(plays) |>
  step_interact(~ likes:video_length) |>
  step_interact(~ shares:video_length) |>
  step_rm(comments)
  

tiktok_wflow4 <- workflow() |>
  add_recipe(tiktok_recipe4) |>
  add_model(linear_reg() |>
            set_engine("lm"))

tiktok_fit_rs4 <- tiktok_wflow4 |>
  fit_resamples(
    resamples = folds, 
    control = control_resamples(save_pred = TRUE, extract = calc_model_stats)
  )
```

```{r}

rmse4 <- collect_metrics(tiktok_fit_rs4, summarize = TRUE) |>
  filter(.metric == "rmse") |>
  select(mean) |>
  rename(mean_rmse = mean)

other_metrics4 <- map_df(tiktok_fit_rs4$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared, na.rm = TRUE), 
            mean_aic = mean(AIC, na.rm = TRUE), 
            mean_bic = mean(BIC, na.rm = TRUE))

combined_metrics4 <- bind_cols(rmse4, other_metrics4)
combined_metrics4
```

We can see that RMSE significantly decreased from about 6.6 million in Model 2 to 6.4 million in Model 4. We also see that adjusted r-squared increased, AIC decreased, and BIC decreased. All of these signs point to Model 4 being a better model in both fit and prediction. Therefore, we will remove comments, and add interactions between video_length and both shares and likes.

#### Conditions for Inference

Residual Plots for Followers vs. Predictors:

```{r}
create_individual_residual_plot <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .fitted, y = .resid)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    labs(title = paste("avg", predictor, "per user")) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 5),
      axis.text.y = element_text(size = 5),
      plot.title = element_text(size = 8),
      axis.title = element_blank(), 
      axis.ticks = element_blank(), 
    )
}

predictor_vars <- c("likes", "shares", "total_videos")
individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, tiktok_users))

combined_plots <- reduce(individual_residual_plots, `+`)

final_plot <- combined_plots + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

We can see from the residual plots that there doesn't appear to be any non-random patterns that violate linearity. So, we can conclude that the linearity condition is satisfied. However, there does appear to be a clear outward fanning spread for each each predictor, meaning that constant variance is not satisfied. To solve this, we can log-transform our response variable (followers) and see if the fanning is minimized:

```{r}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")  

individual_residual_plots <- map(predictor_vars, ~create_individual_residual_plot(.x, dataset_transformed))

combined_residual_plot <- wrap_plots(individual_residual_plots, ncol = 2)

final_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residuals vs. Fitted",
    caption = "Fitted Values",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_plot <- final_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_plot) +
  labs(tag = "Residuals") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

Fanning is a lot less noticeable now. The residual plots don't seem to suggest any underlying patterns. As such, we conclude the predictors satisfy constant variance.

When assessing independence, we know that each of the videos are by individual creators, therefore the videos are independent of each other. There is no reason to believe that one TikTok user's video performance would directly affect another's.

Finally, we assess normality.

```{r}
create_individual_residual_histogram <- function(predictor, data) {
  model <- linear_reg() |>
    set_engine("lm") |>
    fit(reformulate(predictor, response = "followers"), data = data)
  
  augmented_data <- augment(model$fit)
  
  ggplot(augmented_data, aes(x = .resid)) +
    geom_histogram(bins = 40, fill = "blue", color = "black", alpha = 0.5) +
    labs(title = paste(predictor)) + 
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 5),
      axis.text.y = element_text(size = 5),
      plot.title = element_text(size = 12, face = "bold"),
      axis.title.x = element_blank(), 
      axis.title.y = element_blank(), 
      axis.ticks = element_blank(),
    )
}

predictor_vars <- c("likes", "shares", "comments", "plays", "total_videos")

individual_residual_histograms <- map(predictor_vars, ~create_individual_residual_histogram(.x, tiktok_users))

combined_residual_plot <- wrap_plots(individual_residual_histograms, ncol = 2)

final_hist_plot <- combined_residual_plot + 
  plot_layout(ncol = 3, nrow = 2) + 
  plot_annotation(
    title = "Residual Distribution for Each Predictor",
    caption = "Residual",
    theme = theme(
      plot.title = element_text(hjust = 0.5, size = 10, face = "bold"),
      plot.caption = element_text(hjust = 0.5, size = 8, face = "bold")
    )
  ) 

final_hist_plot <- final_hist_plot & 
  theme(
    plot.title = element_text(hjust = 0.5, size = 8),
    plot.subtitle = element_text(size = 10)
  )

wrap_elements(final_hist_plot) +
  labs(tag = "count") +
  theme(
    plot.tag = element_text(size = rel(.8), angle = 90, face = "bold"),
    plot.tag.position = "left"
  )
```

Normality seems to be satisfied for each of the predictors except total_videos and possibly shares. However, even though total_videos and shares do not have completely normal distributions, because we have more than 30 observations in the dataset, we can conclude that normality is satisfied regardless of the distribution.

## Results

After removing likes and video_length, **and adding our interaction term??**, we arrive at our final model:

```{r}
final_fit <- linear_reg() |>
  set_engine("lm")|>
  fit(followers ~ shares + comments + plays + total_videos + likes*total_videos, data=train_transformed)

final_fit |>
  tidy() |>
  kable(digits = 4)
```

Model 4 performance on test:

```{r}
tiktok_fit4 <- tiktok_wflow4 |>
  fit(data = tiktok_train)
# predict on test
tiktok_test_pred4 <- predict(tiktok_fit4, tiktok_test) |>
  bind_cols(tiktok_test)
  
rmse_result4 <- rmse(tiktok_test_pred4, truth = followers, estimate = .pred)
rsq_result4 <- rsq(tiktok_test_pred4, truth = followers, estimate = .pred)

combined_metrics4 <- bind_rows(rmse_result4, rsq_result4)
combined_metrics4
```

```{r}
#| include: false
tiktok_fit3 <- tiktok_wflow3 |>
  fit(data = tiktok_train)

tiktok_test_pred3 <- predict(tiktok_fit3, tiktok_test) |>
  bind_cols(tiktok_test)
  
rmse_result3 <- rmse(tiktok_test_pred3, truth = followers, estimate = .pred)
rsq_result3 <- rsq(tiktok_test_pred3, truth = followers, estimate = .pred)

combined_metrics3 <- bind_rows(rmse_result3, rsq_result3)
combined_metrics3
```

Note that we log transformed our response variable. In order to evaluate the meaning of our RMSE of 0.4067, we take exp(0.4067) \~ 1.502. This value is the multiplicative square difference. For example, if have log followers of 16.04552, our model will be more or less off by $16.04552 \pm .4067^2 \implies \exp(16.04552 \pm 0.1654) \implies 7882219 < 9,299,954 < 10,972,689$. This means our model does a fairly poor at predicting a tiktok user's followers. We also have an RSQ of 0.3615, indicating only 36.2% of the variability in followers can be explained by our predictor variables.

There are several terms that are significant when determining the number of followers a tik tok user has. The number of total videos, comments, and plays seems to have a clear positive relationship with follower count. This also would align with our expectations, as the more videos you make, the more engagement your profile is likely to have and more followers you may gain. However, shares have a negative relationship with follower count, which initially seemed counter-intuitive. While it is impossible for the model to determine causality or explain why exactly a relationship exists, we hypothesize that users may share a video because they dislike it, resulting in them not following the user.

When observing the video length bin variable, the middle video length bin (2) has statistically significant difference from the other two video length bins, as well as a statistically significant interaction term with total videos. This shows that not only do medium length videos generate the most followers, but medium length videos combined with a higher number of total videos significantly increase follower count as well. This is certainly an interesting finding from our analysis, as it isn't the most expected result.

## Discussion + Conclusion

We originally decided to look at TikTok's data and how follower count (a huge driver of engagement) is impacted by other aspects of a user's account. We learned that it is extremely difficult to correctly predict follower count, given our model only captures 36.2% of the variability in the dataset. More complex models seemed to only worsen performance, and we chose to prioritize parsimony for this reason - however, even the simple models did not predict well.

Our dataset was extremely difficult to work with, given that it did not meet the conditions for linear regression (linearity and constant variance), and contained multicollinearity. The variables were also extremely large, and needed to be scaled down to have meaningful coefficients - which made late interpretation significantly more difficult. A more complex model was likely needed, that was beyond the scope of our knowledge, given how poorly our model performed at the end. There also may be underlying relationships between follower count, and other portions of the TikTok algorithm that are not contained in the dataset, which our model might have also failed to capture; in the real world, users have reported that TikTok enforces policies differently from user-to-user, and uses different algorithms from region to region.

In order to improve our analysis, it would be helpful to comb TikTok for a dataset that potentially contains more variables. Three potential options we considered included: finding a meaningful way to capture hashtags (which may require manually looking at TikTok videos), finding a meaningful way to capture whether a user typically utilizes trending music, and using the demographic statistics for users (to account for human decisionmaking).

### Appendix

First 5 data points before transformation

```{r}
head(tiktok)
```

After transformation

```{r}
head(tiktok_users, 5)
```

Split results:

\<a name="appendix"\>\</a\>

```{r}
tiktok_split
```

::: callout-important
Before you submit, make sure your code chunks are turned off with `echo: false` and there are no warnings or messages with `warning: false` and `message: false` in the YAML.
:::
